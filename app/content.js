const projects = {
    "projects": [
        {
            "html": "<p>ByteLight Inc., a Cambridge-area startup, spun out of the Boston University NSF Smart Lighting Engineering Research Center. With expertise in LED-based visible light communication, ByteLight seeks to change the way we use lighting. Energy efficient LED bulbs and lamps provide an opportunity to send wireless data through the visible spectrum itself. Light based communications offers unique advantages over radio based technologies by delivering higher aggregate bandwidth, directional security, and accurate 3D indoor positioning capabilities.</p>\n\t\t\t\n\t\t\t<img src='/images/project_pics/bytelight.jpg' width='600px' align='center'/>\n\t\t\t\n\t\t\t<p>ByteLight has been able to gain traction through their participation in the <a href='http://www.hcp.com/' target='blank'>Highland Capital</a> <a href='http://www.hcp.com/summer/' target='blank'>Summer program</a>. Moving forward, ByteLight is excited to be joining the <a href='http://dogpatchlabs.com/' target='blank'>Dogpatch Labs</a> community. </p>\n\t\t\t\n\t\t\t<p>For more information, feel free to check out the <a href='http://www.bytelight.net' target='blank'>ByteLight Website</a>. ByteLight is currently being led by Aaron Ganick and Dan Ryan.</p>\n\t\t\t<img src='/images/project_pics/woot.jpeg' style='padding-left: 250px'/>", 
            "largePic": "nav/bytelight_lrg.png", 
            "order": 4, 
            "slug": "bytelight", 
            "smallPic": "nav/bytelight.jpeg", 
            "summary": "Optical communications for indoor positioning", 
            "title": "byteLight", 
            "z_subProjects": []
        }, 
        {
            "html": "<div id='header_and_column'>\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n\n<div id='content'>\n\t<p>This page captures my projects built for <a href='http://ng.cba.mit.edu/'>Neil Gershenfeld's</a> Fall 2012 <a href='http://fab.cba.mit.edu/classes/863.12/'>MAS.863 How To Make (Almost) Anything</a> course.</p>\n\n\t<p><a href='http://fab.cba.mit.edu/classes/863.12/people/travis.rich/index.html'>This work is also documented on the fab.cba.mit server.</a> </p>\n</div>", 
            "largePic": "nav/htmaa_lrg.jpg", 
            "order": 11, 
            "slug": "htmaa", 
            "smallPic": "nav/htmaa.jpg", 
            "summary": "How To Make Almost Anything", 
            "title": "howToMake ___", 
            "z_subProjects": [
                {
                    "html": "<div id='header_and_column'>\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n</div>\t\n\n<div id='content'>\n\n<h2> week 1 -- 9/12/12 </h2>\n\n<p>When thinking about potential final projects, it's hard to settle on a single thing. This is especially true knowing that what I know will change over the course of the semster - so predefining what I'll be building at the end seems a bit daunting.</p>\n\n<p>What I have been able to figure out though is that the projects I keep thinking share a few common attributes. In a final project, I'd like to build something that has the following qualities:\n<ul>\n<li>Locomotes</li>\n<li>Has a Personality (face)</li>\n<li>Communication Magic (inspires a feeling of 'how did it know that?')</li>\n</ul>\n</p>\n\n<h4>Mini Monster Military</h4>\n<p>A fleet of tiny (friendly) monsters that can locomote themselves and communicate amongst themselves. A monster can send requests like 'head north!' and all monsters in the troop will move in the same direction. Or perhaps - they could communicate combat moves: 'do an awkward dance!' and they'll all squirm the same goofy dance.</p>\n<img src='/images/htmaa_img/w1_armyOut.jpg' width='600px'/>\n<img class='smallimg' src='/images/htmaa_img/w1_robotAssem.gif' width='250px'/>\n\n<h4>Febrezer</h4>\n<p>Capable of detecting sounds akin to methane generation, localizing the source of the sound and locomoting itself into the afflicted zone to dispense air freshener. </p>\n\n\n<h4>Conversation Anti-Awkwarder</h4>\n<p>Detects sudden drops in the loudness of conversations and is capable of applying social recovery techniques. In 'dinner-conversation' mode, the Anti-Awkwarder will listen to a conversation and detect a sharp drop in noise level - indicating that uncomfortable situation where everyone has run out of witty things to say. At this point, the anti-awkwarder will deliver a controversial statement ('Did you hear they faked the moon landings because JKF was a Scientologist?'), to spark emotions and conversation. </p>\n<p>In 'joke' mode, the anti-awkwarder will listen to conversations to detect sharp drop offs in conversation noise level, indicating a joke falling flat on its face. At this point, the anti-awkwarder will remedy the situation by providing supportive laughter or a sad trombone - depending on it's mood. </p>\n<img src='/images/htmaa_img/w1_TeamBots.png' width='600px'/>\n<img src='/images/htmaa_img/w1_AntiBot.png' width='600px'/>\n\n<h4>Method of Locomotion</h4>\n<p>I'm also interested in building a means of locomotion that is highly entertaining (read: goofy, inefficient, irregular). Standard wheels or treads seem a bit too easy and useful - so I plan to look at some novel ways of moving the little bots. A bit of inspiration comes from Sid's toys in Toy Story. Toys that have been smashed together with whatever is lying around, and as a result, have a unique personality and mode of operation.</p>\n<img src='/images/htmaa_img/w1_sid_toys.jpg' width='600px'/>\n\n<h4>Required Techniques</h4>\n<p>Part of my motivation for this type of project comes from the wide range of capabilities needed. Each of the How to Make modules would be demonstrated. Sensing, output, and communication would be used to let the bots interact with each other and with people - while electronics, 3D printing, and mechanical design would be required for building something that actually moves and functions. </p>\n</div>\n\n</div>", 
                    "order": 6, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week1", 
                    "summary": "blank", 
                    "title": "Proposal"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 7 -- 10/22/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\n\t\t<h4>Introduction</h4>\n\t\t\t<p>This week we're programming the boards we had previously made in <a href='http://fab.cba.mit.edu/classes/863.12/people/travis.rich/week5'>week 5</a>. My board at the time was designed with two LEDS (in the place of eyes) and a single button (in the place of a nose). While there was also the option of building a new Fabduino this week, I stuck with programming my older board, due to the time constraints of the upcoming sponsor week.</p>\n\t\t<h4>Bootloading</h4>\n\t\t\t<p>I first tried bootloading through the Arduino environment, following Dave Mellis' <a href='http://hlt.media.mit.edu/?p=1695'>tutorial</a>. This worked for me using an AVR ISP, but unfortunately I wasn't able to have my device recognized by the Arduino environment when I tried to do the programming in this domain. I switched to a slightly lower level approach to all of this and began to use Neil's quick steps that are <a href='http://academy.cba.mit.edu/classes/embedded_programming/hello.ftdi.44.program.png'>shown here</a>. This eventually worked for me and I was able to get the term.py working - verifying that everything had gone accordingly. I did run into a few issues along the way though. The first seemed to be based on the order in which I connected the ISP and the FTDI cables. I found that unplugging both, then plugging in the ISP 3x2 header and then the FTDI cable gave good results. Other orders seemed to leave me with a 'device not found' error. Another error I kept running into came when running term.py: 'ImportError: No module named tkinter'. I tried a few solutions on my mac side, but eventually gave up, booted into Ubuntu and tried to run term.py from there. I got the same error again, but was able to fix everything by simply installing python-tk: 'sudo apt-get install python-tk'.</p>\n\t\t\t<img src='/images/htmaa_img/w7_prog_0.jpg' width='600px'/>\n\t\t\t<img src='/images/htmaa_img/w7_prog_1.png' width='600px'/>\n\t\t\n\t\t<h4>Programming</h4>\n\t\t<p>I took a first step in programming by editing the <a href='http://academy.cba.mit.edu/classes/embedded_programming/hello.arduino.168.blink.c'>hello.arduino.168.blink.c</a> file. I updated the variables to point to the correct pin on my board and after a small amount of struggle (I didn't realize DDRB needed to be changed to DDRA), my board started blinking! I then tried to get my board to turn on the LEDs with the push of the button. For this, I follow the lead of <a href='http://academy.cba.mit.edu/classes/input_devices/button/hello.button.45.c'>hello.button.45.c</a>. This implemented relatively quickly, but the button seems to have some serious debouncing issues. I got around this by just setting a 1000ms delay after the button is first pressed. This sets the LED to stay on for 1s and gets around any pesky bouncing issues. In future efforts though, an actual debouncer will be used. It also seems like stray capacative effects are triggering the button pin to toggle errantly at times. </p>\n\n\n\t\t\n\t\t<img src='/images/htmaa_img/w7_prog_2.png' width='600px'/>\n\t\t<img src='/images/htmaa_img/w7_prog_3.png' width='600px'/>\n\n\t\t<iframe width='600' height='338' src='http://www.youtube.com/embed/atUk1wg-Bq4' frameborder='0' allowfullscreen></iframe>\n\n\t\t<iframe width='600' height='338' src='http://www.youtube.com/embed/GaGNoKkbK7k' frameborder='0' allowfullscreen></iframe>\n\n\n\n</div>", 
                    "order": 12, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week7", 
                    "summary": "blank", 
                    "title": "ATTiny"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>week 3 -- 9/24/12</h1> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\n\n\t\t<h4>FabISP Intro</h4>\n\t\t\tThis week we're learning how to mill circuit boards, solder components, and program the previously mentioned boards. We'll be making a FabISP (in-system programmer) that will be used for the remainder of the course to program any other boards we go on to design. \n\t\t<h4>Milling</h4>\n\t\t\t<p>I milled my first board directly after the training sessoin, so everything was fresh in my head. I used the older machine, so I had to be a bit careful about the sacrificial board being warped. I tested my z-height in several locations to make sure I would always be hitting the board. In the Fab Modules, I set my 2D z-depth to 0.12 and got nice results from the cut. Instead of using the Modella to cut out the board, I decided to try the pressure chopper. I messed up the board on my first cut by going too slowly. The proper way is to line up the board, hold the board firm, and do a nice quick, chop. By going slowly and not holding the board completely tight, I gave the FR1 a chance to splinter and crack - which it did. On my second iteration, I used the Modella to cut out the board.</p>\n\n\t\t\t<img src='/images/htmaa_img/w3_fabisp_05.jpg' width='600px'/>\n\t\t\t<img src='/images/htmaa_img/w3_fabisp_01.jpg' width='600px'/>\n\n\t\t<h4>Stuffing</h4>\n\t\t\t<p>Soldering all the components onto the board went fairly smoothly. For the components with very small leads (the microprocessor and the usb jack) I made a long solder bridge across all of the leads and then used copper solder wick to pull off the excess. This produced nice results with no bridges and little excess. </p>\n\n\t\t\t<p>The key to soldering these boards, for me, was a good pair of tweezers. Being able to finely manipulate all of the components made the process quite smooth.</p>\n\n\t\t\t<img src='/images/htmaa_img/w3_fabisp_03.jpg' width='600px'/>\n\t\t\t\n\t\t<h4>Programming</h4>\n\t\t\tProgramming I ran into some issues the first n times around. My first mistake was leaving the make file unchanged when trying to program my board with another FabISP. Some tutorial somewhere said to do so when using FabISPs, but it must have been referencing a different Makefile. \n\n\t\t\tI eventually got help from <a href='http://fab.cba.mit.edu/classes/863.11/people/eyal.toledano/howto/home.html'>Eyal Toledano</a> who lent me his commercial product ISP. This worked right away (as expected in hindsight, since the Makefile was setup for that). I used the directions he laid out in his <a href='http://fab.cba.mit.edu/classes/863.11/people/eyal.toledano/howto/week2.html'>Week 2 writeup</a>. The key of this writeup was running the command 'avrdude -c usbtiny -p t44 -P usb -U flash:w:main.hex:i' once everything was connected.  \n\n\t\t\tOnce everything was programmed, I verified the success of everything by first checking that my computer registered a connected USB device with the name 'FabISP' and second by programming another students board with my board. \n\n\t\t\n\t\t\n\t\t<img src='/images/htmaa_img/w3_fabisp_04.jpg' width='600px'/>\n\t\t\n\n\n</div>", 
                    "order": 8, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week3", 
                    "summary": "blank", 
                    "title": "Fab ISP"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\t\n\n<div id='content'>\n\t<h2>Week 4 -- 10/01/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\t<p>This week we're getting to know our loved z-axis and exploring 3D scanning and printing. There's bonus points if we can 3D scan an item and then print it (this is apparently non-trivial, as created a watertight 3D from a scan can be tricky at times). </p>\n\n\t\t<h4>Printing</h4>\n\t\t<p>This week I'm using the printing assignment to further a bit of research that is already ongoing. I'm interested in designing computational surfaces and textures and I've been exploring the mechanisms in which these can be manufactured. To test the domain of 3D printing textures, I wanted to create a sample piece that would demonstrate the minimum features sizes that can be produced by the Invision printer available in the CBA shop.</p>\n\n\t\t<p>I started with a 1' cube and added an array of features to it' top facet. I based my range around the stated minimum DPI capabilities of the machine: 328x328x606 DPI (x,y,z). These yield minimum features of .003' and .0017' in the x,y, and z directions, respectively. Given the different capabilities in x,y versus z, I decided to also explore the minimum capabilities of defining textures on a vertical plane of the cube. I copied the pattern from top and mapped it onto the back plane of the cube. </p>\n\n\t\t<p>The cube came out of the printer completed. Immediately, I noticed some of the small individual posts that were created have become bent, mushed, or broken. Fotunately, the textured patterns that were packed closley together seemed to remain in tact and well maintainted. It seems like the dense clustering and wax filling were suffient support. Furthermore, the features printed on the side of the cube appear to have remained more in tact, suggesting that the scaffolding provides better support in that plane. The cube was put in the oven for about an hour to melt the excess wax. This process removed most of the wax scaffolding, but tiny bits of wax in between the small features remained. I was given the advice to put the piece in a warm corn oil bath (~175 F). After this process, the remaining wax was removed. Cleaned, it became clear to see that many of the smaller, individual features did not survive. This information is useful in understanding the true limits of what can be acheived with the machine (beyond the simple DPI specification).</p>\n\n\t\t<img src='/images/htmaa_img/w4_3d_1.jpg' width='600px'/>\n\t\t<img src='/images/htmaa_img/w4_3d_2.jpg' width='600px'/>\n\t\t<img src='/images/htmaa_img/w4_3d_3.jpg' width='600px'/>\n\n\t\t<h4>Scanning</h4>\n\t\t<p>For the scanning assignment I used the NextEngine Desktop scanner. I scanned over 9 intervals at a macro distance of 6.5'. The software aligned the scans well and the only real work I had to do to clean up the model was fuse the scans and then fill some remaining holes. The back leg of the LEGO man seems to have been the hardest part to scan (least data points), which could be explained by the black holes and small features that it contains.</p>\n\n\t\t<p>When fusing holes, I found that Meshlab was much faster than the NextEngine Scanner software. Given that, it's advisable to export the scanned mesh as a .STL and perform the minor fixes in Meshlab, rather than trying to perform the entire workflow in the NextEngine software.</p>\n\t\t<img src='/images/htmaa_img/w4_3d_5.jpg' width='600px'/>\n\t\t<img src='/images/htmaa_img/w4_3d_6.png' width='600px'/>\n\t\t<img src='/images/htmaa_img/w4_3d_7.png' width='600px'/>\n\t\t<img src='/images/htmaa_img/w4_3d_8.png' width='600px'/>\n\n</div>", 
                    "order": 9, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week4", 
                    "summary": "blank", 
                    "title": "3D"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n</div>\t\n\n<div id='content'>\n\t<h2>week 2 -- 9/17/12</h2>\n\n\t\t<h4>Ideation</h4>\n\t\t<p>I've had the same eyeglass frames since I was 12. Frames that are too small for my head, make me look like Harry Potter, and have that annoying bend in them from being sat on in 9th grade. So for this week's project I decided to make a press-fit glasses kit. While the idea started out as a bunch of different frame styles, it quickly devolved into the 3rd grade giddyness of a Mr. Potato-head style kit. I set out to build a glasses kit with attachable eyebrows, mustaches, and wonky frames. The Harry Potter thing has gotten me pretty far, so why no try out Mr. Potato Head.</p>\n\t\t<img src='/images/htmaa_img/w2_pressfit_00.png' width='600px'/>\n\t\t<h4>Design</h4>\n\t\t<p>Before cutting out each of the parts, I ran a few test cuts to determine a nice press-fit slot width. I got advice to shoot around 4mm from the MAS863 mailing list, but tested a range of sizes from 3 to 5 mm. I decided I liked the feel of the 3.9mm slot the best, but it seemed like everything from 3.8 to 4.2 gave a pretty snug fit. To get the general dimensions of the different parts, I took a ruler and roughly measured my facial features: the length from the top of my nose to the side of my head, length from the side of my head to the back of my ear, and the width and length of my nose for the mustache attachments. </p>\n\n\t\t<p>The final kit contains frames, eyebrows, mustaches, temple arms, a linear connector to serve as the bridge and connector to the eyebrows, and a perpendicular connector to attach the temple arms and mustaches.</p>\n\t\t<img src='/images/htmaa_img/w2_pressfit_01.JPG' width='600px'/>\n\t\t<img src='/images/htmaa_img/w2_pressfit_02.JPG' width='600px'/>\n\t\t<h4>Cutting</h4>\n\t\t<p>I did my initial designs using InkScape. When getting ready to lasercut, I used the Ubuntu partition and used the fab modules to generate the .path and .epi files to send to the Epilog lasercutter. There was an annoying effect that the location of my design was mirrored vertically and horizontally in the bed of the lasercutter - i.e. my design in the top left corner would cut in the bottom right corner of the lasercutter. It was also a bit tedious to jump from InkScape to .bmp to fab modules to laser printer. After a little while, I switched onto the Windows partition and used Corel Draw for the remainder of the project. I had to export the design as a .ps file from InkScape and import into Corel Draw (.cdr format by default). Exporting as a .eps, .svg, and .svgz all incorrectly imported into Corel Draw (the node relationships weren't maintained).</p>\n\t\t<p>The default settings for cardboard left my cuts incomplete in certain points. I wound up iterating and trying several different values, but found the best results with speed:55, power:35, and frequency:1200.</p>\n\n\n\t\t<h4>Output</h4>\n\t\t<!-- <img src='/images/htmaa_img/w2_pressfit_03.JPG' width='800px'/> -->\n\t\t<img src='/images/htmaa_img/w2_pressfit_06.JPG' width='600px'/>\n\t\t<img src='/images/htmaa_img/w2_pressfit_04.JPG' width='600px'/>\n\t\t<img src='/images/htmaa_img/w2_pressfit_03.JPG' width='600px'/>\n\t\t<img src='/images/htmaa_img/w2_pressfit_05.JPG' width='600px'/>\n\t\t<img src='/images/htmaa_img/w2_pressfit_09.JPG' width='600px'/>\n\t\t<img src='/images/htmaa_img/w2_pressfit_10.JPG' width='600px'/>\t\t\n\t\t<img src='/images/htmaa_img/w2_pressfit_07.JPG' width='600px'/>\n\t\t<img src='/images/htmaa_img/w2_pressfit_08.JPG' width='600px'/>\n\n\n</div>", 
                    "order": 7, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week2", 
                    "summary": "blank", 
                    "title": "Press Fit"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 12 -- 11/27/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\t<h4>Introduction</h4>\n\t\t\t<p>This week we're learning to write applications and user interfaces. I decided to use the light sensor board I built in the past as the hardware. I decided to build out visualizations that depict the light intensity measured by the board in a friendlier way than a simple bar graph. I'll use buttons to switch between the different visualizations, allowing me to play with making buttons as well. </p>\n\t\t<h4>Serial Interface</h4>\n\t\t\t<p>I began writing this week's assignment in python, using Neil's <a href='http://academy.cba.mit.edu/classes/input_devices/light/hello.light.45.py'>light input code</a> as a baseline. I decided that using TKinter was a bit too widgety for what I wanted to build this week, so I started to play with the idea of using Processing to parse the serial port input and for the visualization. The trickiest part here was porting Neil's code (or rather the algorithm it steps through) for reading serial port data into Processing. Processing has an extra layer of abstraction that tricked me for a little bit as I spent time expecting to get raw binary values. In reality, processing outputs the decimal values by default. Once I figured this out, it was relatively straight forward to perform header detection and to grab the byte values representing the light intensity from the board. I did have one other misstep that caused the program to run very slowly - I forgot to clear the serial port buffer after each read. This led to the code getting backed up with old serial data, which resulted in the application appearing laggy. </p>\n\t\t\t<img src='/images/htmaa_img/w12_inter00.jpg' width='600px'/>\n\t\t<h4>UI</h4>\n\t\t\t<p>I wrote two visualtions, a tree and Timmy. The tree's growth (how full it is) is proportional to the amount of incident light. More light = healthier, fuller tree. The second visualtion is Timmy. Timmy is terrified of the dark. His reaction changes in response to how much ambient light he (the hardware board) can sense. Implementing the buttons in processing is a bit of a trick. Rather than have a button element, you define the regions of the button and on each update check the position of the mouse. If you mouse is within the defined coordinates and a mouseClick is detected, you implement actions that would be expected of a click. In this case, each click changes which display mode the interface is in, and updates the buttons' color to reflect state.</p>\n\n\n\t\t\t<p>Note that the below video has inverted colors on the monitor to make it easier to see (the black on white background was a bit washed out on camera). </p>\n\n\t\t\n\t\t<img src='/images/htmaa_img/w12_inter01.png' width='600px'/>\n\t\t<img src='/images/htmaa_img/w12_inter02.png' width='600px'/>\n\t\t<img src='/images/htmaa_img/w12_inter03.png' width='600px'/>\n\n\t\t<iframe width='600' height='338' src='http://www.youtube.com/embed/FzT6oTHVyF0' frameborder='0' allowfullscreen></iframe>\n\n\n</div>", 
                    "order": 17, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week12", 
                    "summary": "blank", 
                    "title": "Interface"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>week15 -- 12/11/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t<p>I burn my tongue on food far too frequently. I need a buddy who can help. </p>\n    <img src='/images/htmaa_img/w15_final_18.jpg' width='600px'/>\n\n    <h4>Building my Final Buddy</h4>\n    <img src='/images/htmaa_img/w15_final_01.png' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_02.png' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_03.png' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_04.png' width='600px'/>\n    <iframe width='800' height='450' src='http://www.youtube.com/embed/4MTgzpvTacY' frameborder='0' allowfullscreen></iframe>\n    <img src='/images/htmaa_img/w15_final_05.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_06.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_07.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_08.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_09.jpg' width='600px'/>\n    <iframe width='800' height='450' src='http://www.youtube.com/embed/8LXvQBNBx5w' frameborder='0' allowfullscreen></iframe>\n    <img src='/images/htmaa_img/w15_final_10.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_11.jpg' width='600px'/>\n    <iframe width='800' height='450' src='http://www.youtube.com/embed/yuwTUlnHAqs' frameborder='0' allowfullscreen></iframe>\n    <img src='/images/htmaa_img/w15_final_12.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_13.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_14.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_15.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_16.jpg' width='600px'/>\n    <iframe width='800' height='450' src='http://www.youtube.com/embed/vJoNMcNJE_c' frameborder='0' allowfullscreen></iframe>\n    <img src='/images/htmaa_img/w15_final_17.jpg' width='600px'/>\n    <img src='/images/htmaa_img/w15_final_18.jpg' width='600px'/>\n    <iframe width='800' height='450' src='http://www.youtube.com/embed/Zzl9-Jgvk8Y' frameborder='0' allowfullscreen></iframe>\n    <iframe width='800' height='450' src='http://www.youtube.com/embed/vbb960LTteM' frameborder='0' allowfullscreen></iframe>\n\n\n\n    <hr>\n\n\t<h4>Proposal Questions</h4>\n\t\t<h4>what will it do?</h4>\n\t\t<p>My temperature buddy will have a temperature sensor that can detect if food is too hot (or cold) to eat. If the food is too hot, the temperature buddy will shake violently and its face will slowly glow a nice burning red. If the food is too cold, the forehead will glow blue denoting imminent brainfreeze and its head will shake with a chill.</p>\n\t\t<img src='/images/htmaa_img/w15_sample.jpeg' width='600px'/>\n        <h4>who's done what beforehand?</h4>\n        <p>The most common thing that's done beforehand, is people have burned their tongues. There do exist common <a href='http://www.bachelorkitchenblog.com/wp-content/uploads/2011/12/stickthermometer.jpg'>food thermometers</a> but these are rarely used in practice. There have been projects that try to build thermistor-based food thermometers, as discussed in <a href='http://forums.adafruit.com/viewtopic.php?f=8&t=25096'>this thread</a>, thought I haven't found any documentation of finished projects.</p>\n        <h4>what materials and components will be required?</h4>\n        <p>I will need the following materials and components:</p>\n        <ul>\n        \t<li>Wood/3D Printer Material/or Acrylic to create a structure for the head</li>\n        \t<li>Felt cloth or some material to create the external 'skin' of the head</li>\n        \t<li>Thermistor and associated hardware to create temperature sensor</li>\n        \t<li>Motor to control head movements</li>\n        \t<li>LEDs to illimunate 'skin' based on food temperature</li>\n        \t<li>Batteries, wiring, and connectors to power and connect all the electronics</li>\n        \t<li>Googly Eyes</li>\n        \t\n        </ul>\n        <h4>where will they come from?</h4>\n        <p>Many of the electronic components are available in the Fab Inventory. I have several motors that can be used to drive the movement. I have some spare acrylic that I can use to build a prototype of the head structure, and I can use wood/3D printing to create a final version if deemed worthwhile.</p>\n        <p>The fabric to encase the exterior of the buddy is not currently available at the lab, but I will be able to buy some at local craft shops.</p>\n        <h4>how much will it cost?</h4>\n        <p>I expect the fabric used for the external skin to cost a couple tens of dollars.</p>\n        <h4>what parts and systems will be made?</h4>\n        <ul>\n        \t<li>Body/Head Structure of the Temperature Buddy</li>\n        \t<li>Temperature sensing circuitry</li>\n        \t<li>LED system driven by temperature sensor reading</li>\n        \t<li>Motor driver and motor driven by temperature sensor reading</li>\n        \t<li>External fabric 'skin' to cover the temperature buddy</li>\n        </ul>\n        <h4>what processes will be used?</h4>\n        <ul>\n        \t<li>3D CAD design of body/head structure</li>\n        \t<li>Milling Circuit Boards</li>\n        \t<li>Pressfit Design for head structure</li>\n        \t<li>Lasercutting head structure</li>\n        \t<li>Embedding programming to drive motors/LEDs/sensors</li>\n        </ul>\n        <h4>what tasks need to be completed?</h4>\n        <p>The tasks outlined in the section 'what parts and systems will be made' need to be completed. In further detail, this included designing the head CAD model, designing the circuit boards that drive the LEDs, motors, and sensors, and designing an output scheme to convert the sensor readings into appropriate outputs </p>\n        <h4>what questions need to be answered?</h4>\n        <p>What is the dynamic range of the temperature setting? How quickly will the temperature sensor cool off (denoting the food is no longer too hot to eat)? </p>\n        <h4>what is the schedule?</h4>\n        <p>I plan to spend the last couple days of this week building the sensor, LED, and motor hardware and associated software. Once that functionality is verified, and the dimensions of the components are known, I will build the structure of the head/mouth. Final touch up and details will be made towards the end of the weekend and Monday.</p>\n        <h4>how will it be evaluated?</h4>\n        <p>The temperature buddy has one main goal (and two minor goals). The main goal is to accurately notify me when food is too hot. This means that the sensor will appropriately measure the temperature and output a signal (face red, head shaking no). In addition, the temperature buddy will be evaluated based on its ability to denote if food is too cold, and to denote when food has sufficiently cooled down and is ready to eat. The timing characteristics of this are important, so quantitative results on the timing curves for temperature response may be a useful measurement to document.</p>\n</div>", 
                    "order": 20, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week15", 
                    "summary": "blank", 
                    "title": "Final"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 8 -- 10/28/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\t<h4>Introduction</h4>\n\t\t\t<p>This week we were tasked with 'making something big'. We were trained on the large bed shopbot and the waterjet cutter and supplied with rigid insulation foam (2' X 8') and OSB panels. We were encouraged to use other materials (aluminum, glass, plywood) if we could get a hold of it. </p>\n\n\t\t<h4>Design</h4>\n\t\t\t<p>After passing up on molding a dinosaur a few weeks ago, I decided this would be a good chance to go back to that idea. Walking by the excellent dinosaur skeleton Andy Bardagjy made last year, I decided I'd build an 'living' dinosaur to differentiate a bit (i.e. not a skeleton). I found a .STL model of a nice herbivore and used AutoDesk's 123D-Make software. The software allows you to upload a 3D cad model and presents options for you to segment the design for either layered or interlocking pieces. I decided to use the insulation foam and to make a layered design. The foam is 2' thick, so it was thick enough to easily fill the full volume of the 3D model. 123D-Make output a .eps file which I imported into AutoCad and exported as a .dxf file. The .dxf can then be imported into Partworks which builds a toolpath for the shopbot.</p>\n\t\t\t<img src='/images/htmaa_img/w8_big_01.png' width='600px'/>\n\t\t\t<img src='/images/htmaa_img/w8_big_02.png' width='600px'/>\n\t\t<h4>ShopBot</h4>\n\t\t\t<p>Starting the shopbot was fairly straightforward and Tom provided a lot of guidance. My initial design fit onto about 1.2 pieces of foam when packing the pieces closely together. Tom advised using two full pieces of foam though and spacing the parts loosely. After zeroing the x, y, and z axes and screwing down the foam boards, I started the job. Everything went smoothly until my first small piece was cut out. After the piece was fully cut, the vacuum used to pick up sawdust and debris had enough suction to lodge the small piece loose and pull it up a bit. When this happened, the piece would usually get hit by the end mill and get stuck between the end mill and the plastic dust cage. The first time this happened, I hit the emergency stop button as a first reaction. Apparently what I should have done though, given that this wasn't necessarily an immediate safety issue, was to click the software stop button. By clicking the emergency stop button, the shopbot job lost it's place and I had manually edit the .sbp file to get back to the point when I hit the emergency stop button. From this point on, I had to quickly hit the software stop button at the end of each small piece, remove the piece (hopefully before much damage was done to it) and then resume the job. </p>\n\t\t\t<img src='/images/htmaa_img/w8_big_03.png' width='600px'/>\n\t\t\t<img src='/images/htmaa_img/w8_big_04.png' width='600px'/>\n\n\t\t\t<img src='/images/htmaa_img/w8_big_05.jpg' width='600px'/>\n\t\t\t<img src='/images/htmaa_img/w8_big_06.jpg' width='600px'/>\n\t\t\t<img src='/images/htmaa_img/w8_big_07.jpg' width='600px'/>\n\t\t\t\n\t\t<h4>Assembly</h4>\n\t\t\t<p>To assemble the pieces I began by using a 3M spray adhesive. I tested the spray on a small piece of foam first, to ensure that it wouldn't erode the foam and to test the strength of the bond. The test did erode the foam a small amount, but the bond was sufficiently strong that I decided to move forward. Assembly went smoothly until the tail portion. The balance of the body was such that it was hard to hold the pieces in place while the glue set. I eventually began using small dabs of hot glue on the underside of the joints to create a stronger bond. The hot glue would prove useful again when I got to the neck portion as these pieces were strongly off balance. On some pieces of the neck I also opted to stick small stiff wires from the bottom through a few of the pieces. At the end, the model seemed to hold together quite well as I carried to around and moved it about. </p>\n\n\n\t\t\n\t\t\n\n\t\t<img src='/images/htmaa_img/w8_big_09.jpg' width='600px'/>\n\t\t<img src='/images/htmaa_img/w8_big_10.jpg' width='600px'/>\n\t\t<img src='/images/htmaa_img/w8_big_11.jpg' width='600px'/>\n\n\n\n</div>", 
                    "order": 13, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week8", 
                    "summary": "blank", 
                    "title": "Big"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 14 -- 12/11/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\t<p>This week we're looking machine design and are building the components to make a automated magnetic chess table. This week is a group effort, and I worked with <a href='http://fab.cba.mit.edu/classes/863.12/people/salzberg/index.html'>Shaun Salzberg</a>, <a href='http://fab.cba.mit.edu/classes/863.12/people/austin.lee/index.html'>Austin Lee</a>, <a href='http://fab.cba.mit.edu/classes/863.12/people/jinjoo/HowToMake/Home.html'>Jin Joo Lee</a>, and <a href='http://fab.cba.mit.edu/classes/863.12/people/sujoy/index.html'>Sujoy Chowdhury</a>. My role this week was to design and fabricate the support beams that attached to the threaded robs and move the magnetic pedestal across the playing area. A video of the machine working is given below. The project is part of Shaun's final project. His great writeup on the work completed this week can be <a href='http://fab.cba.mit.edu/classes/863.12/people/salzberg/week14.html'>found here</a>.</p>\n\n\t\t<iframe width='800' height='450' src='http://www.youtube.com/embed/HlHZgGYabgA' frameborder='0' allowfullscreen></iframe>\n\n\n\n\n</div>", 
                    "order": 19, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week14", 
                    "summary": "blank", 
                    "title": "Machine"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 5 -- 10/09/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\t<h4>Schematic</h4>\n\t\t\t<p>This week we're copying the <a href='http://academy.cba.mit.edu/classes/embedded_programming/hello.ftdi.44.png'>echo hello-world board</a> and redesigning the layout from scratch - and, while we're adding it, adding a couple components to the circuit. I decided to add a couple LEDs and button. They're roughly laid out in the shape of eyes and a nose, so when you press the nose - the eyes light up. Copying the circuit over was fairly simple, however, I noticed that Eagle - unlike other circuit schematic programs, won't show little loop hops when two traces grahically cross but aren't electrically connected. This caused some confusion in the double-checking process. The two LEDs I added are hooked up in parallel with 1K resistors in series with each. </p>\n\t\t\t<img src='/images/htmaa_img/w5_pcb_01.png' width='600px'/>\n\n\t\t<h4>Layout</h4>\n\t\t\t<p>For the layout I decided to place my parts on a 1' radius circle to stick with the loose face theme I had with the eyes and nose. I used a minimum trace width of 12 mils, and a minimum clearance distance of 20 mils between wires. I made the mistake of not also defining a minimum pad-to-wire distance and stuck with the default of 8 mils. When I milled the board, three of the pads were connected with a wire and I had to use an exacto knife to separate them manually. </p>\n\n\t\t\t<p>Doing the layout with only a single layer can be really tricky and after a little while of not finding a routing topology that worked, I opted to use a couple 0-ohm resistors to let me hop over existing traces. These simplified much of my layout and made the process a bit nicer. </p>\n\t\t\t<img src='/images/htmaa_img/w5_pcb_03.jpg' width='600px'/>\n\n\t\t<h4>Mill and Stuffing</h4>\n\t\t\t<p>The milling and stuffing steps are similar to <a href='http://fab.cba.mit.edu/classes/863.12/people/travis.rich/week3/'>week3</a>. I made the mistake of making the dimension layer too thick in Eagle when doing the layout and this caused the Modela to try to mill out a thick cut. Rather than a single line width outline, it made a 2 or 3 line width outline. This didn't have any real consequence besides causing the job to take more time. </p>\n\n\t\t<img src='/images/htmaa_img/w5_pcb_04.jpg' width='600px'/>\n\t\t\n\n\t\t\n\t\t<img src='/images/htmaa_img/w5_pcb_05.jpg' width='600px'/>\n\n</div>", 
                    "order": 10, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week5", 
                    "summary": "blank", 
                    "title": "PCB"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 6 -- 10/15/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\t<h4>Intro</h4>\n\t\t\t<p>This week we're tackling molding and casting. For my model, I stumbled upon <a href='http://www.nasa.gov/multimedia/3d_resources/models.html'>NASA's great archive of 3D models</a> and decided to make a sapce shuttle. This decision came given the constraints we have with the milling process. We have a 3-axis milling machine, which limits us to having any models with interior features. I also decided to make a 1-sided mold, which further constained me to choose a model that did not have significant 'bottom' features. </p>\n\t\t\t<img src='/images/htmaa_img/w6_cast_01.png' width='600px'/>\n\t\t<h4>Milling</h4>\n\t\t\t<p>I milled my board using the Modela miller with a .125' ball-tip bit. The roughing and finishing runs estimated that they would take 4.5 hours to complete, but everything was finished in about 3 hours. The Modela wax shavings were much larger than I had expected, but the machine didn't seem to mind - plowing right through all the excess wax. I did vacuum the excess every hour or so, just so that I could visually check that everything was going smoothly.  </p>\n\t\t\t<img src='/images/htmaa_img/w6_cast_02.jpg' width='600px'/>\n\t\t\t<img src='/images/htmaa_img/w6_cast_03.jpg' width='600px'/>\n\t\t\t<img src='/images/htmaa_img/w6_cast_04.jpg' width='600px'/>\n\t\t\t\n\n\t\t<h4>Molding</h4>\n\t\t\t<p>For molding we used the Oomoo silicone rubber kit. I measured the two components out by weight, and followed the prescribed ratio of 1A:1.3B. I mixed and shook the two parts for about 10 minutes after combined. I attempted to remove as many air bubbles as possible by shaking and dropping the cup on the table. When pouring, I also tried to keep the stream fine as an attempt to remove any additional bubbles. I followed this procedure until the entire surface of my model was covered, and then poured a bit more quickly, assuming that air bubbles within the cast - but not on the surface- wouldn't be much of a concern. Once the entire mold was filled, I shook it for another 10-15 minutes, trying to bang out any remaining air pockets. After an hour or so, the top layer of the mold was solid, but still a bit tacky. After 2.5 hours, the mold felt much more solid, but I gave it another couple hours just to be safe. </p>\n\t\t\t<img src='/images/htmaa_img/w6_cast_05.jpg' width='600px'/>\n\n\t\t<h4>Casting</h4>\n\t\t\t<p>I began casting with the drystone. The mixing went well, but when demolding, the wings and tailwing snapped off. They were too thin and the material too brittle. I could imagine being sufficiently delicate that this doesn't happen on a second run - but I opted to try a harder casting material. I used the HydroStone Super X for the second run. I also put tape around the edge of my cast so that I could have a thicker top layer to begin with. This run came out a bit better - the wings still intact. However, the tail still snapped on when (delicately) trying to pull it out. My hunch is that the aspect ratio of the tail combined with it's thickness just makes it a very tricky thing to reproduce.</p>\n\n\t\t\n\t\t\n\t\t<img src='/images/htmaa_img/w6_cast_06.jpg' width='600px'/>\n\n\n\t\t\n\t\t<img src='/images/htmaa_img/w6_cast_07.jpg' width='600px'/>\n\t\t<img src='/images/htmaa_img/w6_cast_08.jpg' width='600px'/>\n\n\n</div>\n\n\t\n\n\n</body>\n</html>", 
                    "order": 11, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week6", 
                    "summary": "blank", 
                    "title": "Casting"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 10 -- 11/12/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\t<h4>Introduction</h4>\n\t\t\t<p>This week we're playing with composites. Let's make something super light and super strong! I started from the idea of wanting to make a 'glove'. A composite that outlines the shape of a hand with spiraling fabric. Something like a boxer's wrap held in place, but without the hand actually being there. </p>\n\t\t<h4>Twine</h4>\n\t\t\t<p>I started trying to make this hand by wrapping twine. I filled a Nitrile glove with water, tied it off, and laid it flat. Unfortunately, the twine, because of the way it's packaged, had some natural curvature to it that made wrapping the glove quite difficult. After fussing with the twine for a while and realizing it was going to look terrible - I dropped the idea. I decided to move on to trying it with an easier material.</p>\n\t\t<h4>Second Try: Fabric</h4>\n\t\t\t<p>This attempt I cut lots of fabric strips about a half-inch wide and 8 inches long. I soaked these in West System 105/205 epoxy mix (5 pumps from each). When the strips were sufficiently soaked through, I began slowly wrapping each of the fingers with the thin fabric. This was made easier by the fact that the soaked resin helped the fabric to stick to itself as I wrapped. I kept pressure on the glove to keep water forced into the fingers, giving me a good amount of rigidity to work with. It was rather difficult to wrap some of the undersides of the fingers, as everything quickly descended to a tacky surface, but with a little stubbornness, it was finished. I sealed my bag and connected the vacuum.</p>\n\t\t\t<p>On vacuuming, there was the unanticipated effect that the fingers became more squashed that the palm of the glove (I anticipated the vacuum to put pressure on the glove, keeping the fingers rigid - but I didn't anticipate the non-uniform pressure). THis resulted in some of the wrap becoming deformed and the fingers coming out flatter than desired. Also, I had initially intended to pop the glove and remove the excess material, leaving me with the shell of the hand only. Because of the deformation though, I decided that the glove would be a bit nicer looking with the water-glove filling. Not to mention, the hand had a nice feel to it with the glove and composite wrap.</p>\n\t\t\t<p>The glove quickly took the role of a high-five buddy. Giving me a friendly outlet for each of my small daily successes. </p>\n\n\n\t\t<img src='/images/htmaa_img/w10_epoxy_01.jpg' width='600px'/>\n\t\t<img src='/images/htmaa_img/w10_epoxy_02.jpg' width='600px'/>\n\t\t<img src='/images/htmaa_img/w10_epoxy_03.jpg' width='600px'/>\n\t\t\n\t\t<iframe width='600' height='338' src='http://www.youtube.com/embed/P4_xSPxEXWw' frameborder='0' allowfullscreen></iframe>\n\t\t<img src='/images/htmaa_img/w10_epoxy_04.jpg' width='600px'/>\n\n</div>", 
                    "order": 15, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week10", 
                    "summary": "blank", 
                    "title": "Epoxy"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 11 -- 11/20/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\t<h4>Introduction</h4>\n\t\t<p>This week we're tasked with driving some output form a microcontroller board. I had an old 7 segment LED board lying around from last semester and I decided I wanted to finally make it work. Oh - it's huge. It's over half a foot tall. I think it will also be used as part of the display in my final project - so we get to kill two birds with one stone here.</p>\n\t\t\n\t\t<h4>Hardware</h4>\n\t\t<p>I decided to make the board with a <a href='http://fab.cba.mit.edu/content/projects/fabkit/'>fabduino</a>. That was straight forward to fabricate. The tricky part which I didn't realize until after I made the fabduino (and before I decided on the 7seg LED) was that I was going to need a shift register to drive all of the pins necessary. Given this hindsight, and the little amount of time I had (ahh crit day stress!), I had to, for the time being, resort to using a breadboard to jump between my fabduino and 7-segment LED display. I used a 8-bit serial-in parallel-out shift register (<a href='http://www.sparkfun.com/datasheets/IC/SN74HC595.pdf'>74HC595</a>). This was fairly straight forward to use, and if you're in the Arduino environment, there's a <a href='http://www.arduino.cc/en/Tutorial/ShiftOut'>nice tutorial</a> to step you through the process.</p>\n\n\t\t<img src='/images/htmaa_img/w11_output_01.png' width='600px'/>\n\t\t<img src='/images/htmaa_img/w11_output_02.png' width='600px'/>\n\t\t\n\t\t<h4>Programming</h4>\n\t\t<p>Once everything was setup, I referred to the 7-segment display's datasheet to reference the pin mapping. My initial goal was to cycle from 0-9 to demonstrate the functionality. Unfortunately, after mapping out all of the numbers, I found that the display was driving completely wrong segments. After a good amount of debugging, I realized that the datasheet's pinout diagram was incorrect. I reverse engineered each pin and segment individually and then remapped my numbers from that. The mapping that wound up working is as follows (these are the binary values sent to the register):\n\t\t\t<ul>\n\t\t\t\t<li>0 - 0b11111110</li>\n\t\t\t\t<li>1 - 0b00111111</li>\n\t\t\t\t<li>2 - 0b01000101</li>\n\t\t\t\t<li>3 - 0b00010101</li>\n\t\t\t\t<li>4 - 0b00110011</li>\n\t\t\t\t<li>5 - 0b10010001</li>\n\t\t\t\t<li>6 - 0b10000000</li>\n\t\t\t\t<li>7 - 0b00111101</li>\n\t\t\t\t<li>8 - 0b00000001</li>\n\t\t\t\t<li>9 - 0b00110001</li>\n\t\t\t</ul>\n\t\tFrom that point, everything was relatively straight forward, and the code has a simple delay counter that can be changed to vary how long each number is displayed for. </p>\n\n\n\t\t<img src='/images/htmaa_img/w11_output_03.png' width='600px'/>\n\t\t<img src='/images/htmaa_img/w11_output_04.png' width='600px'/>\n\n\t\t<img src='/images/htmaa_img/w11_output_05.jpg' width='600px'/>\n\t\t<iframe width='600' height='338' src='http://www.youtube.com/embed/2JYp8Se98zw' frameborder='0' allowfullscreen></iframe>\n\n\n\n</div>", 
                    "order": 16, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week11", 
                    "summary": "blank", 
                    "title": "Output"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 13 -- 12/02/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\t\n\t\t<h4>Introduction</h4>\n\t\t\t<p>This week we're building devices that network - or, more simply put, a project with two or more processors. I'm dabbling with the idea of making data flashlights for my final project, so I decided to play a bit with using using visible light LEDs for communication.</p>\n\n\t\t<h4>Hardware</h4>\n\t\t\t<p>I used two boards for this project: a fabduino and the light sensor board that I had made a few weeks ago. The fabduino served as the transmitting board and the phototransistor on the light sensor board made the obvious choise as the receiver board. For this week I decided to stay with a simplex system, that is - I can only transmit in one direction (i.e. it's broadcast). The fabduino didn't have a bright LED on it, so rather than making a new board, I simply hooked up an LED and 1k resistor to one of the output ports. </p>\n\t\t\t<img src='/images/htmaa_img/w13_net_00.jpg' width='600px'/>\n\t\t\t\n\t\t<h4>Communication</h4>\n\t\t\t<p>To transmit, I set up a software serial port on the pin tied to my LED and had the loop continuously write 'hello' to the serial port. The result is a simple on-off keying modulation of the LED. The serial port was set to 9600 baud. The receiver side performed simple serial reads to receive the message. The challenge here was running into a lot of bit errors and timing issues. Sometimes the entire message would be bit shifted leaving me with a garbled message (and non-ASCII output). I got a lot of flipped bits as well due to shadows and other slight fluctuations in light intensity. This would be a good argument against on-off keying as a modulation scheme. To work around this, I implemented a few hacky error-checks that were run before passing the data to the display interface. One check is to delay the display until the same message is received 5 times in a row. This ensures that quick bit flips don't cause the output to glitch. This doesn't solve the bit shifting issue though. To avoid this, I implemented a check that made sure the data received was in the ASCII range of A-Za-z. If the data was outside this range, I bit shift by one and check again - repeating until the data is verified to be A-Za-z.</p>\n\t\n\t\t\n\t\t<img src='/images/htmaa_img/w13_net_01.png' width='600px'/>\n\t\t<img src='/images/htmaa_img/w13_net_02.jpg' width='600px'/>\n\n\t\t<iframe width='600' height='338' src='http://www.youtube.com/embed/N6vi1io5SiU' frameborder='0' allowfullscreen></iframe>\n\t\t<iframe width='600' height='338' src='http://www.youtube.com/embed/8-g6kkxOU18' frameborder='0' allowfullscreen></iframe>\n\n\n\n\n\n</div>", 
                    "order": 18, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week13", 
                    "summary": "blank", 
                    "title": "Network"
                }, 
                {
                    "html": "<div id='header_and_column'>\n\n\t\t<a href='http://travisrich.com/htmaa/week1'><div class='column_text'><div class='circular' id='week1'></div><div class='navtext'>week 1 Proposal</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week2'><div class='column_text'><div class='circular' id='week2'></div><div class='navtext'>week 2 Press Fit</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week3'><div class='column_text'><div class='circular' id='week3'></div><div class='navtext'>week 3 FabISP</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week4'><div class='column_text'><div class='circular' id='week4'></div><div class='navtext'>week 4 3D</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week5'><div class='column_text'><div class='circular' id='week5'></div><div class='navtext'>week 5 PCB</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week6'><div class='column_text'><div class='circular' id='week6'></div><div class='navtext'>week 6 Casting</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week7'><div class='column_text'><div class='circular' id='week7'></div><div class='navtext'>week 7 ATtiny</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week8'><div class='column_text'><div class='circular' id='week8'></div><div class='navtext'>week 8 Big</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week9'><div class='column_text'><div class='circular' id='week9'></div><div class='navtext'>week 9 Input</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week10'><div class='column_text'><div class='circular' id='week10'></div><div class='navtext'>week 10 Epoxy</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week11'><div class='column_text'><div class='circular' id='week11'></div><div class='navtext'>week 11 Output</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week12'><div class='column_text'><div class='circular' id='week12'></div><div class='navtext'>week 12 Interface</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week13'><div class='column_text'><div class='circular' id='week13'></div><div class='navtext'>week 13 Network</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week14'><div class='column_text'><div class='circular' id='week14'></div><div class='navtext'>week 14 Machine</div></div></a>\n\t\t<a href='http://travisrich.com/htmaa/week15'><div class='column_text'><div class='circular' id='week15'></div><div class='navtext'>week 15 Final</div></div></a>\n\n</div>\t\n\n<div id='content'>\n\t<h2>Week 9 -- 11/05/12</h2> <!-- Normally '<h1>Week 1 -- Final Project Proposal</h1>' -->\n\n\t\t<h4>Introduction</h1>\n\t\t\t<p>This week we're focusing on input devices. I decided to make a frequency detector with the photodiode circuit that Neil put up as a demo. The photodiode would receive incident modulated light (modulated somewhere between 100Hz and a few kHz) and would be able to determine either the frequency (to some accuracy) or whether or not the incident frequency was within a certain range. </p>\n\t\t<h4>Board Design and Build</h4>\n\t\t\t<p>I used the <a href='http://academy.cba.mit.edu/classes/input_devices/light/hello.reflect.45.png'>board</a> that Neil used in his synchronous detection demo example. Milling went as planned, but while stuffing the board, I noticed that the phototransistors available in the electronics room were different than the one in the picture of Neil's original board. I wound up using that transistor, but putting it on backwards (since I had to guess which side was the cathode of the original part - we had no schematic to work from). This became evident after programming the board and running the python interface. The phototransistor reacted very little to variations in light. I flipped the component and all was working smoothly from then on. </p>\n\t\t\t<img src='/images/htmaa_img/w9_input_00.jpg' width='600px'/>\n\t\t\t<img src='/images/htmaa_img/w9_input_01.png' width='600px'/>\n\t\t<h4>Frequency Detection</h4>\n\t\t\t<p>I began testing the circuit as is to observe if the 'diff' value varies in it's behavior as a function of the frequency of the incident light. I figured that given the filtering window we were using, there should be some resonant frequencies that produce much larger swings in the read-out value. I connected an LED to a function generator and shined the LED towards the phototransistor. I varied the frequency of the LED and did indeed observe variation in the behavior of the ouput as a function of frequency (see the video below). </p>\n\n\t\t\n\n\t\t<iframe width='600' height='338' src='http://www.youtube.com/embed/sVolQEhrkGk' frameborder='0' allowfullscreen></iframe>\n\n</div>", 
                    "order": 14, 
                    "parent": 11, 
                    "pic": "blank", 
                    "slug": "week9", 
                    "summary": "blank", 
                    "title": "Input"
                }
            ]
        }, 
        {
            "html": "<h4>Innovators Guild: Detroit - Lighting Challenge</h4>\n<ul id='toc'>\n\t<li><a href='#mission'>The Mission</a></li>\n\t<li><a href='#intro'>The Mindset Going In</a></li>\n\t<li><a href='#day1'>First Day</a></li>\n\t<li><a href='#day2'>Second Day</a></li>\n\t<li><a href='#day3'>Third Day</a></li>\n\t<li><a href='#takeaway'>A Few Take Away Messages</a></li>\n\t<li><a href='#presentation'>Presentation</a></li>\n\n</ul>\n\n<h4 id='mission'>The Mission</h4>\n<p>As part of the 2012 MIT Media Lab Innovators Guild, I travelled to Detroit to join a team of researchers. Our focus was on lighting around the city. Our first meeting began by explicitly detailing that this was going to be a different kind of challenge. Advisors made it clear that we weren't going to fix anything - we likely weren't going to prototype anything substantial. Those who had already spent some time in Detroit assured us that the challenges that exist in Detroit are not the kind that can be solved in a weekend hackathon. The charge given to us was to develop relationships. To meet local Detroiters, listen to their stories and interpretation of the challenges that exist, and to begin to develop trust. </p>\n\n<p>The entire Media Lab group was split into five different challenges. The challenge I was placed in was Lighting. Detroit has 80,000 public street lights - 40,000 of which don't work. Many residents have complained that the darkness is a problem that leads to a host of other serious issues. My team was comprised of Valentin Heun (MIT Media Lab), Jeff Chapin (IDEO), David A. Smith (Lockheed Martin), Jeff Sturges (OmniCorp Detroit), and Jim Young (Xunlight). There was a strong mix of engineers, designers, and varying professional experiences.</p>\n\n<p>The onset problem statement given to us (which was left intentionally vague) was to explore possible paths for improving the street lighting conditions in Detroit. We were encouraged to tweak this statement over the course of the trip. </p>\n\n<table class='image'>\n<caption align='bottom'>A broken Detroit Light.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_00.jpg' width='600px'></td></tr>\n</table>\n\n<h4 id='intro'>The Mindset Going In:</h4>\n\n<p>As we would soon find out from our first interactions with Detroiters, outsiders - or aliens as a few people called them - were actually a problem. People who would come to Detroit with the idea that their power, money, and influence could 'fix' Detroit. They'd usually be there for a few days and then be gone, never to be seen again - with the newfound ability of claiming that they've been to Detroit as though it were a badge of honor. These alien trips don't accomplish anything and have developed a knee-jerk reaction among many Detroit citizens to distrust outsiders. This trip was all about developing trust and showing the local people that the Lab was sincere in their commitment to make our effort more than a quick weekend 'badge of honor'. </p>\n\n\n\n<table class='image'>\n<caption align='bottom'>Members of the lighting team (excluding myself), joined by Detroiter Clif (far right). From left to right: David A. Smith, Valentin Heun, Jeff Chapin, and Jeff Sturges (back facing camera).</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_01.jpg' width='600px'></td></tr>\n</table>\n\n\n\n<p>Arriving on the first day we were shuttled to our hotel in the heart of downtown Detroit. Despite the feeling of a regular downtown financial district, the number of vacant buildings and lack of people was striking. Someone gave the figure that Detroit proper was designed for 2.5 million people and the current population was only around 700,000. </p>\n<table class='image'>\n<caption align='bottom'>A surprisingly empty Downtown intersection. The GM building stands prominently in the background.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_02.jpg' width='600px'></td></tr>\n</table>\n\n<h4 id='day1'>First Day:</h4>\n<p>Our first day consisted mostly of wandering around the city and getting a feel for a few of the neighborhoods (Our plane landed around 1:00pm). We met up with several people at OmniCorp Detroit that gave us further tours of the city. One of my first impressions was that there was an inspiring sense of freedom in the city. You have the opportunity to plot your own course, create your own community, determine your destiny - and often times in ways that violate city laws, but because of the lowered oversight you can get away with it for long enough that it eventually becomes a real part of the community (e.g. taking over local abandoned lots for urban farms). This 'freedom' seems to come with a heightened risk though. The lack of externally enforced infrastructure means more dangerous (feeling) neighborhoods and lots of failure, which though not inherently bad, can lead to tougher times.</p>\n<table class='image'>\n<caption align='bottom'>A Detroit street corner.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_03.jpg' width='600px'></td></tr>\n</table> \n\n<table class='image'>\n<caption align='bottom'>Outside OmniCorp Detroit</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_04.jpg' width='600px'></td></tr>\n</table>\n\n<p>The night ended with a kick-off dinner led by Joi and Colin Raney(IDEO). Here, they outlined the five task areas (Lighting, Soil, Agriculture, Air, and Digital Community) again and gave us an opportunity to meet with several of the local Detroiters to further explore the challenges ahead of us. </p>\n<table class='image'>\n<caption align='bottom'>Meeting with local Detroiters and our challenge team members.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_05.jpg' width='600px'></td></tr>\n</table>\n\n<h4 id='day2'>Second Day:</h4>\n<p>Our second day began by traveling to a community garden where we met Wayne and Myrtle. We pulled up and were surprised to find them with a group of ~20 neighborhood kids. They were all working together to paint a mural for their neighborhood. Though shy at first, they were very open and friendly and open to explaining what they were working on. </p>\n<table class='image'>\n<caption align='bottom'>Manistique Street Community Garden. Run by Wayne and Myrtle.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_06.jpg' width='600px'></td></tr>\n</table>\n\n<table class='image'>\n<caption align='bottom'>Local kids painting a mural.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_07.jpg' width='600px'></td></tr>\n</table>\n\n<p>Wayne spoke to us for a bit and conveyed a very deep sense of connection with the land that they were planting on. It was here that I first felt that Detroit had somehow taken a step back (not necessarily in a negative way) to a time when humans had a much stronger relationship with the earth they live on. Detroit began to feel like a series of small tribes (neighborhoods) that had an inspiring sense of self-dependence. It felt like a human society that existed before the time of mass consumerism. The focus is on family, community, self sustenance - but now with the added advantage of the internet, cell phones, and IT. </p>\n<table class='image'>\n<caption align='bottom'>Wayne speaking to us about his garden and community.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_08.jpg' width='600px'></td></tr>\n</table>\n\n<p>Myrtle, his wife, echoed these sentiments. Though with less of a spiritual tone, she also conveyed the idea that the community was the base unit by which Detroit functioned. Block-committees were setup and met frequently to determine efforts, programs, and initiatives that might be valuable for their neighborhoods. She also gave us an important anecdote regarding lighting. The streetlight across from her house had been broken for some time. She had actively petitioned the local government to fix the light for over two years. They finally got around to her request and fixed the light, only to have it be shot out with a BB gun two weeks later. Two years of effort for two weeks of light. This represents the larger problem, which many people detailed, that the local government was either too slow or too disconnected to have any meaningful impact on the lives of the Detroit citizens. </p>\n\n<p>Later that day we explored the Mt. Elliot MakerSpace - a hackerspace Jeff Sturges has setup in the basement of the Mt. Elliot Church to work with kids and local community members. Jeff made clear his inspiration from Neil Gershenfeld's fab lab movement and compared the MakerSpace to a low-budget fab lab targeted at a younger audience. There we had the fortunate coincidence of bumping into Kevon - one of Jeff Sturges' 17 year old students (currently a senior in High School). He sat with us for several hours, talking about his experiences with the lighting conditions in Detroit. He mentioned that a lot of the time, dogs - rather than humans - were a problem in darker areas. Aggressive dogs have been known to attack people who accidentally stumble too close, often because they hadn't seen the dog in the first place. Damien, another Detroiter that works with Jeff at the MakerSpace also joined us. He had strong ideas about how he could use education and learning as a means of empowering people to fix the problems they experienced in Detroit (rather than working for a top-down fix). Jeff Sturges commented after these discussions that he was shocked by how effective the design-focused approach that was used with Kevon turned out to be. He said that the interaction had a strong effect on him and would influence how he approaches problem-solving with some of the younger kids in the MakerSpace.</p>\n<table class='image'>\n<caption align='bottom'>The group talking with Kevon (left) and Damien (right).</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_09.jpg' width='600px'></td></tr>\n</table>\n\n<p>That night, our timing was fortunate enough to coincide with DeLectricity - an art inspired event with a heavy focus on light. There was a bike parade through part of town where hundreds of people gathered to ride through the city. Given it was part of the DeLectricity festivities, people were strongly encouraged (and provided with materials) to outfit their bikes with bright lights and colorful reflector tape. One group, the East Side Riders, said that they themselves run a bike group that tricks out their bikes to be overly bright and flamboyant for the purposes of riding around the neighborhood and acting as deterrents to crime. </p>\n<table class='image'>\n<caption align='bottom'>DeLectricity Light Bike event. Centered are the East Side Riders.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_10.jpg' width='600px'></td></tr>\n</table>\n\n<p>A strong take away message that I took from the first day was that you can't fix a community from the outside. If you want to influence a community, you must either 1) join the community and make a full time commitment, or 2) help in knowledge exchange and let those in the community carry your knowledge to fix their problems. Furthermore, there are a lot of well meaning people outside the community who want to help, but can't - so giving them a channel where they can put their effort is valuable. A local institution (e.g. hackerspace) can develop a relationship within the community and then have outsiders come in and help with the knowledge exchange. This way residents trust the local institution and the aliens are able to contribute knowledge exchange by participating at the hackerspace.</p>\n\n\n<h4 id='day3'>Third Day:</h4>\n<p>The third day we spent a lot of time in the morning going over what we had learned from the day before and brainstorming what we could in the time remaining that would be of value. We quickly came to the conclusion that simply building a light would be a wasted effort that didn't live beyond the span of the weekend. We slowly gravitated towards the idea of running a lighting workshop out of the Mt. Elliot MakerSpace. We planned to have a handful of kids build their own lighting devices, with the idea that it would demonstrate that they had the capability of influencing their direct lighting situation. We gathered a bunch of lighting equipment (flashlights, solar panels, electroluminescent panels and wire, lawn lights, and bulbs) and brought it all to the makerspace. </p>\n<table class='image'>\n<caption align='bottom'>The Lighting team's morning brainstorm.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_11.jpg' width='600px'></td></tr>\n</table>\n\n<table class='image'>\n<caption align='bottom'>Idea board with some of the approaches brought up by the Lighting team.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_12.jpg' width='600px'></td></tr>\n</table>\n<p>When we arrived, several kids were already there using the space for their own purposes. We began to unpack the equipment and were attempting to make up a plan of how the workshop would look on the fly. We began deconstructing the flashlights to get access to the LEDs they had inside and before we could finish this step, or come up with a coherent plan for the workshop, the kids gathered around with enthusiasm. They all started asking if they could help. Out of this, the workshop organically grew. The kids took apart the flashlights, had fun seeing how everything connected, and at our suggestion, got excited to rebuild the lights in their own form. We began to show them how to solder together the parts of the flashlight and they quickly picked it up - adapting the form factor of the flashlight to their liking. While there was a clear learning goal in our minds, the kids took it much more as a game, and quickly started building the lights for the purpose of playing laser tag with each other. It was surprising how differently they approached the workshop compared with what we had expected would happen. They were much more engaged and creative than we would have predicted. It was really like our only job was to bring the materials and provide a little amount of guidance - they took care of everything else. </p>\n\n<table class='image'>\n<caption align='bottom'>Betty after making her reflective cape.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_13.jpg' width='600px'></td></tr>\n</table>\n\n<table class='image'>\n<caption align='bottom'>Betty and Raven learning how to connect the LEDs to the battery.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_14.jpg' width='600px'></td></tr>\n</table>\n\n<table class='image'>\n<caption align='bottom'>Raven showing off her work.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_15.jpg' width='600px'></td></tr>\n</table>\n\n<table class='image'>\n<caption align='bottom'>One of the East Side Riders building a new light for his bike with Jeff.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_16.jpg' width='600px'></td></tr>\n</table>\n\n<p>The last day all the groups presented their work to the rest of the IG members and to a handful of local Detroiters that came to listen. The focus was on recapping the experience with an eye towards how this can move forward. We presented the logistics of what we did, but also some of the stronger take away messages that coalesced by the end of the trip. See the <a href='#presentation'>Presentation</a> section for a link to our slides.</p>\n\n<table class='image'>\n<caption align='bottom'>Joi starting off the presentation session</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_17.jpg' width='600px'></td></tr>\n</table>\n\n<h4 id='takeaway'>A Few Take Away Messages</h4>\n<p>The lighting problem, at its core, is really a safety problem. While not having street lights may make a neighborhood look worse, the real issue here is when a lack of lighting leads to dangerous situations. This could be both a matter of being seen (to avoid being hit by cars) and seeing your surroundings (to acknowledge a dangerous situation). What we found was that many of the neighborhoods, being unable to rely on local government to fix the lights, developed a stronger sense of community to combat safety concerns. It seemed like if you can't have a well-lit street, a good backup was a community that feels safe and is there to protect you in times of trouble. Furthermore, we left with the feeling that a big impact can be made by empowering the locals to build their own lighting solutions. Even if they couldn't make something as bright or efficient as what a larger municipal infrastructure could provide, the act of performing a service for the community would serve as a gesture that strengthens the relationships and trust amongst its members. </p>\n\n<p>We also noticed that there was a interesting difference between public and private space in Detroit. Generally, public spaces contain one set of rules for behavior and private spaces contain enough. In many cities, the rules in a public space are well maintained because of the presence of government law enforcement and infrastructure maintenance. In Detroit, however, the lack of influence of local government and the damaged state many public spaces have fallen into has led to a situation where the rules of behavior in public spaces are less defined. This often has negative side effects - a lowered sense of safety, poorly maintained facilities, etc. As a result, many people in Detroit have tried to reclaim public spaces as their own private spaces. Private spaces which they can maintain and keep safe. A simple way we saw people doing this was by using bright porch lights that throw light well into the street. This combats the lighting issue, but also lays claim, in some sense, to wherever the light reaches. Similarly, people have put lights in trees and other areas of their neighborhood, not for the purpose of lighting, but to claim that space as area that someone cares about - sending a message to people who pass. The Heidelberg project is a fantastic example of this. The Heidelberg project is a multi-block area that was taken over by one person and decorated with huge amounts of artistic pieces. The art, regardless of its artistic value, claims the space as something that is tended and cared for by a local resident - sending the message that the crumbling structures that tend to be prevalent in government controlled areas won't be found there.</p>\n<table class='image'>\n<caption align='bottom'>One of the houses in the Heidelberg project.</caption>\n<tr><td><img src='/images/detroit_imgs/detroit_18.jpg' width='600px'></td></tr>\n</table>\n\n\n<p>Another theme that our team came to was this idea of redefining reality. The reality of life for people living in Detroit is very different than that of many other people in the country. For an outsider, it can be very difficult to fully understand or realize the reality that people in Detroit experience. As such, many of the solutions and products that are made to 'fix' Detroit are made in another reality - a reality that completely neglects many of the subtleties that create issues in Detroit. As such, these solutions fail. Often times though, whether it be from the local government or other outsides, these solutions given to the people of Detroit are all they have. Thus, many people in Detroit are simply presented with a broken reality. Something that doesn't - and can't - work to solve their problems. In many cases, the only way to solve this problem may be to empower the locals to realize that they have the power to shape their own reality - to deconstruct that which is given to them, and to rebuild it in into their own functioning thing that works for their reality. This was something that we saw happening in the workshop as the kids made their own flashlights. The ones given to them were very finished and presented as a final product - a flashlight. But really, it is merely a set of batteries, LEDs, and some connecting wires. By showing this to the kids, they were able to deconstruct the flashlight and build it into their own reality - a game of lasertag. Even though the end product they made was no different in physical functionality than the flashlight they started with - the fact that it had been constructed from their reality, rather than an outside one, made all the difference. Empowering people to realize that that which is presented to them is not a single thing, but rather, a collection of components that can be deconstructed and reconstructed, is a message that we think could have a powerful influence on many people in Detroit.</p> \n\n<h4 id='presentation'>Presentation</h4>\n<p>The presentation that we gave to the IGDetroit group and local residents on our final day in Detroit can be <a href='/static/files/lightingPresentation.pdf' target='blank'>found here</a>.</p>", 
            "largePic": "nav/detroit_lrg.jpg", 
            "order": 20, 
            "slug": "igdetroit", 
            "smallPic": "nav/detroit.jpg", 
            "summary": "summary", 
            "title": "igDetroit", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>Bopscotch is a tweak on the classic hopscotch game. Each square of the court is associated with a specific tone, allowing a player to create a melody, or team up with others to compose complex melodies, as they hop over the court. By layering tones onto hopscotch, sound is mapped on top of space which strengthens connections between spatial and aural awareness.</p>\n\n<iframe src='//player.vimeo.com/video/65671335' width='600' height='338' frameborder='0' webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n\n<p>Bopscotch was made for the OTHER Festival at the MIT Media Lab in Spring 2013 by <a href='http://hidenise.com/' target='blank'>Denise Cheng</a>, <a href='http://shaunsalzberg.com/' target='blank'>Shaun Salzberg</a>, and myself.</p>", 
            "largePic": "nav/bopscotch_lrg.jpg", 
            "order": 17, 
            "slug": "bopscotch", 
            "smallPic": "nav/bopscotch.jpg", 
            "summary": "summary", 
            "title": "bopscotch", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>Jack Chick makes some ridiculous comics (ridiculous in the hateful, extremist sense). Conspiracy theorists make ridiculous websites (in the hateful, comic sans sense). So, naturally, we combined the two.</p>\n\n<p>The overtly ridiculous demon-child is found at <a href='http://lowgradecondoms.s3-website-us-east-1.amazonaws.com/' target='blank'>this demon url.</a></p>\n\n<p>LGC comics is a system that randomly generates conspiracy theorist text and inserts it into randomly selected Chick Tract comics. Auto-generated covers and titles complete the package.</p>\n\n<img src='/images/project_pics/lgc_1.jpg' width='600px'/>\n\n<p>The project owes a serious debt to <a href='http://www.truthforhumanity.com/'>Truth for Humanity</a>, <a href='http://garfieldminusgarfield.net/'>Garfield Minus Garfield</a>, the Internet, and of course, to Jack Chick, whose work we parody without permission.</p>\n\n<p>The project was built by Chia Evers, <a href='http://education.mit.edu/people/jason-haas' target='blank'>Jason Haas</a>, <a href='https://twitter.com/novysan' target='blank'>Dan Novy</a>, <a href='http://www.savannahniles.com/' target='blank'>Savannah Niles</a>, and myself.</p>", 
            "largePic": "nav/lgc_lrg.jpg", 
            "order": 19, 
            "slug": "lgc", 
            "smallPic": "nav/lgc.jpg", 
            "summary": "summary", 
            "title": "lgcComics", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>For my thesis work at the MIT Media Lab, I'm building technologies to create a self-descriptive universe. To provide context, this work is being done in the Viral Spaces group; an ever-evolving creature. From its origins in viral communication and radio systems, it has grown to explore mobile device ecologies, their influence on our physical spaces, and most recently, their ability to create meaningful experiences in our everyday lives. Building on this past experience, the group has transitioned from designing communicative spaces to creative spaces. Spaces that enable experiences that are instructive, creative, and engaging.  To that end, I'm exploring how we can leverage the 'classic' part of our environments - the passive objects that don't have built in radios, sensors, and batteries - to create these spaces.</p>\n\n<iframe src='http://player.vimeo.com/video/63745204' width='600' height='337' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n\n<p>The internet-of-things is an idea that suggests the connection of all of our gizmos; if it plugs into a wall, it should plug into the network. That way, my oven can talk to my toaster, which can talk to my fridge and turn on my lights which react to my alarm, all of which are controlled by my phone. But, what about other things? What about passive things: my hat, my shirt, my a car engine, a bike chain. What would these things have to say, if we could make them talk. Perhaps they would tell you where they were made, or how people use them, or what their CAD file is so that we can 3D print another copy. They would become self-descriptive.</p> \n\n<p>In a self-descriptive universe, all of our objects reveal themselves to you: how they're made, who uses them, how they're used, how they can be fixed, what principles and concepts guide their function. To begin researching how such a world can be built, I'm leveraging advancements in digital manufacturing to create objects with fine surface features that encode data. This data can be read with common cell-phone cameras and acts as a map, describing exactly what you're looking at and providing hooks to collected information about the object. Designing the encoding patterns, manufacturing techniques, and network backend to deliver this experience is the focus of my thesis, and the first steps to imbuing passive objects with the technology to act as tools for creating more social, engaging, and creative spaces.</p>\n\n<p>More formally, I propose a method for encoding surfaces with many identifying markers through the use of deterministic physical features - i.e. deterministic textures. These textures will be designed such that they are 1) machine readable and 2) sufficiently small such that they do not interfere with the original use of the object (be it functional or aesthetic). I propose the design and construction of a device that can decode these physical markers and relay the read data to a device for user interaction. Whereas barcodes, QR codes, and RFID tags are often used to associate a single object with a single piece of data, this technique of encoding surfaces will allow for many points of identification to be placed on a single object, enabling applications in learning, group interaction, and gaming.</p>\n\n<p>My slide deck that I presented during the public Media Lab Crit-day are below. </p>\n<iframe src='http://www.slideshare.net/slideshow/embed_code/15579147?rel=0' width='600' height='487' frameborder='0' marginwidth='0' marginheight='0' scrolling='no' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' allowfullscreen \nwebkitallowfullscreen mozallowfullscreen> </iframe> \n\n<p>The final submitted thesis can be <a href='/static/files/travisRich_thesis_2013.pdf' target='blank'>downloaded here</a>. </p>\n\n<img src='/images/project_pics/encoded_01.png' width='600px'/>\n<img src='/images/project_pics/encoded_02.png' width='600px'/>\n<img src='/images/project_pics/encoded_03.png' width='600px'/>\n<img src='/images/project_pics/encoded_04.png' width='600px'/>\n<img src='/images/project_pics/encoded_05.png' width='600px'/>", 
            "largePic": "nav/encoded_lrg.jpg", 
            "order": 15, 
            "slug": "encodedsurfaces", 
            "smallPic": "nav/encoded.jpg", 
            "summary": "blank", 
            "title": "encodedSurfaces", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>The Mirai Hackathon was hosted at the MIT Media Lab at the beginning of Fall 2013. Bringing artists from Japan, Media Lab students, and industry designers together, we built. Working with Daito Manabe, exploring the future of entertainment, we developed the game Twitch.</p>\n\n<iframe src='//player.vimeo.com/video/74883808' width='600' height='338' frameborder='0' webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n\n<p>Twitch is a twist on the classic jumping platformer game. Rather than a single person experience, the game tightly couples two people into a cooperative play. One player uses and mouses to jump from platform to platform. This player makes requests for where they wish for a platform to appear. These requests generate electric shocks on the fingers of player two. Each finger corresponds to an associated button. Pressing the button corresponding to the shocked finger generates the platform and allows the first player to stay alive. Death, by missing a platform or hitting a monster, results in shocks to the second players face.</p>\n\n<p>The electrodes used to shock Player 2's face are precisly placed so as to trigger the fine muscles located around their face, resulting in electrically stimulated facial expressions. Face tracking and an EEG sensor are used to determine the focus and smile of player 2 and dictates the speed of the game scrolling.</p>\n\n<p>Twitch was created by <a href='fuzzywobble.com' target='blank'>Alex Fuzzywobble</a>, Taishi Kamiya, Sakapon Keiho, <a href='http://www.daito.ws/en/' target='blank'>Daito Manabe</a>, Travis Rich, <a href='http://www.dsworld.net/' target='blank'>Dan Sawada</a>, and <a href='http://cwwang.com/' target='blank'>Che-Wei Wang</a></p>\n\n<img src='/images/project_pics/twitch_01.jpg' width='600px'/>\n<br>\n<img src='/images/project_pics/twitch_02.jpg' width='600px'/>\n<br>\n<img src='/images/project_pics/twitch_03.jpg' width='600px'/>\n<br>\n<img src='/images/project_pics/twitch_04.jpg' width='600px'/>\n<br>\n<img src='/images/project_pics/twitch_05.jpg' width='600px'/>\n<br>\n<img src='/images/project_pics/twitch_06.jpg' width='600px'/>\n<br>\n<img src='/images/project_pics/twitch_07.jpg' width='600px'/>\n<br>\n<img src='/images/project_pics/twitch_08.jpg' width='600px'/>\n<br>\n<img src='/images/project_pics/twitch_09.jpg' width='600px'/>\n<br>\n<img src='/images/project_pics/twitch_10.jpg' width='600px'/>", 
            "largePic": "nav/twitch_lrg.jpg", 
            "order": 22, 
            "slug": "twitch", 
            "smallPic": "nav/twitch.jpg", 
            "summary": "summary", 
            "title": "twitch", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>AudioFile overlays imperceptible tones on standard audio tracks to embed digital information that can be decoded by standard mobile devices. AudioFile lets gives users explore their media more deeply by granting them access to a new channel of communication. The project creates sound that is simultaneously meaningful to humans and machines. Movie tracks can be annotated with actor details, songs can be annotated with artist information, or public announcements can be infused with targeted, meaningful data.</p>\n\n<p>This project was done in collaboration <a href='mailto:sjsu@mit.edu' target='blank'>Stephanie Su</a>.</p>\n\n<img src='/images/project_pics/audio_1.jpg' width='600px'/>\n<img src='/images/project_pics/audio_2.jpg' width='600px'/>\n<img src='/images/project_pics/audio_3.jpg' width='600px'/>\n\n\n<p>Two simple audioCode demos are shown below. One that brings up actor information, ... </p>\n<iframe width='600' height='450' src='http://www.youtube.com/embed/gZ49g4x2CBo' frameborder='0' allowfullscreen></iframe>\n\n<p>... and one that translates public announcements.</p>\n<iframe width='600' height='338' src='http://www.youtube.com/embed/BGeFlfK3fEI' frameborder='0' allowfullscreen></iframe>", 
            "largePic": "nav/audiofile_lrg.jpg", 
            "order": 9, 
            "slug": "audiocodes", 
            "smallPic": "nav/audiofile.jpg", 
            "summary": "Imperceptible tones to embed digital information.", 
            "title": "audioCodes", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>For a monthly party, a tradition was started of creating admission coins. The coins are used to make sure only members of the community are given access, food, and drink. As a side-project of my thesis work, we decided to make encoded coins. The coins are fabricated with a series of cavities that encode 32 bits of information. The coins can be read and decoded by a custom-made coin reader (see video below).</p>\n\n<img src='/images/project_pics/coin_00.jpg' width ='600px'/>\n\n<p>The coin is scanned each time a party-goer wants to get more food or drink. By having each person use a uniquely identifiable (anonymized) coin, we're able to track drinking and eating behavior over the course of the night. Drinking too much towards the end of the night? The bartender can be notified and advise slowing down. More playfully though, we intend to iterate in future months with more entertaining uses for the coin. Perhaps the 1000 'customer' gets to (is forced to) wear a king's crown for the night. </p>\n\n\n\n<img src='/images/project_pics/coin_01.jpg'/>\n<iframe width='600' height='338' src='http://www.youtube.com/embed/6hRXu5tQc68' frameborder='0' allowfullscreen></iframe>\n\n<img src='/images/project_pics/coin_02.png' width ='600px'/>\n<img src='/images/project_pics/coin_03.png' width ='600px'/>\n<img src='/images/project_pics/coin_04.jpg' width ='600px'/>\n<img src='/images/project_pics/coin_05.jpg' width ='600px'/>\n\n\n<iframe width='600' height='338' src='http://www.youtube.com/embed/MC87Y0b5bng' frameborder='0' allowfullscreen></iframe>", 
            "largePic": "nav/coin_lrg.jpg", 
            "order": 14, 
            "slug": "99coins", 
            "smallPic": "nav/coin.jpg", 
            "summary": "blank", 
            "title": "99Coins", 
            "z_subProjects": []
        }, 
        {
            "html": "<h4>Beginnings</h4>\n<br>\n<p>As a follow up to a May hackathon, a Media Lab sponsor flew <a href='http://www.leithinger.net/'>Daniel Leithinger</a> and me to China to work directly with their sample-production factory for a week. We went in with the baseline goal of improving upon an RFID sensing bag we originally made at the hackathon. By working closely with the factory workers, we could design the hardware and they could design the textiles to integrate cleanly.</p>\n\n<p>The high level goal of the trip was to design and build interactive bags. Bags that are designed for a world where the things we carry are not simply books and paper, but tablets and laptops (and looking to the future, smart clothing, smart tools, smart etc). How will our bags evolve as the things we carry evolve?</p>\n\n<p>We bought a bevy of parts we thought we would need to build these: Arduino boards, LED displays and tapes, electroluminescent panels, zigbee radios and chips, a mini oscilloscope, hand tools, lots of wire and cable, accelerometers, GPS chips, hall effect sensors, ambient light sensors, RFID readers and tags, surface-mount cameras, memory chips, breadboards, and handful of other miscellaneous components. </p>\n\n<p>We had a close-knit feedback loop between our designs and those of the factory workers. We would propose a hardware build, figure out the bag requirements, have the sewers propose several sewing and build options, and then tweak our original designs to fit their processes. It worked out quite well and interfacing at each step of the build let us create some finely integrated packs.</p>\n\n<p>Below of photos of the factory, parts room, and conference room from where we worked. </p>\n\n\n<div class='image_block'>\n\t<img width='600px' src='/images/backpack_imgs/factory_01.jpg' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/factory_02.jpg' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/factory_03.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/factory_04.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/factory_05.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/factory_06.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/factory_07.JPG' alt=''/>\n</div>\n\n\n<br>\n<h4>The Bags</h4>\n<br>\nIn total, we built seven bags: \n<ul>\n\t<li><a href='#bag_1'>RFID pack</a></li>\n\t<li><a href='#bag_2'>RFID Luggage bag</a></li>\n\t<li><a href='#bag_3'>Fridge-light pack</a></li>\n\t<li><a href='#bag_4'>Two Friendship packs</a></li>\n\t<li><a href='#bag_5'>Ambient-light pack</a></li>\n\t<li><a href='#bag_6'>Camera pack</a></li>\n\t\n\t\n</ul>\n<br>\n<h4 id = 'bag_1'>RFID Backpack</h4>\n<br>\n<p>The first bag we made was a re-hash of the simple RFID pack we hacked together earlier in May. This time, we could finely integrate the LED readout and RFID readers into the fabric of the bag. We worked from an existing bag model that was designed to carry a laptop, tablet, and other electronics. The ultimate goal is to have a RFID antenna encompassing the entire opening of the bag, but because we didn't have the time or resources to make a custom RFID antenna, we used off the shelf readers and placed them around the bag. There are a total of five readers in the bag. One main reader was located at the top of the bag. At this reader any item can be scanned in or out of the bag. We put a lower reader within the laptop sleeve to determine the presence of the laptop. A reader in the tablet sleeve to determine the presence of a tablet. And finally, two readers in the small front pockets to determine the presence of your wallet and phone. </p>\n\n<p>The bag has seven LEDs embedded in its right strap. Six of the lights are used to read out the presence of certain items, and a seventh white light is used as a power indicator. The LEDs are RGB addressable, so we have fine grain control of the color and intensity of each LED. The LEDs will light green or red to let you know whether a certain item is in or out of the bag (respectively).</p>\n\n<p>The bag is fitted with an accelerometer and piezo buzzer. When a tag is scanned at the main RFID reader, the buzzer beeps, letting the user know their tag was read. The accelerometer tracks the bags movement. Under certain situations, the accelerometer data will trigger the lights on the bag to turn off (e.g. when the bag has been resting for a long period of time). Shaking the bag will wake it up and turn the notification LEDs back on. </p>\n\n<p>All of the electronics and batteries in the bag fit into a small case placed in a side pocket.</p>\n\n<div class='image_block'>\n\t<img width='600px' src='/images/backpack_imgs/rfid_00.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/rfid_01.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/rfid_02.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/rfid_03.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/rfid_04.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/rfid_05.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/rfid_06.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/rfid_07.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/rfid_08.JPG' alt=''/>\n</div>\n\n<div class='video_block'>\n\t<iframe width='600' height='338' src='http://www.youtube.com/embed/fTLxCVC8eDQ' frameborder='0' allowfullscreen></iframe>\n</div>\n\n<br>\n<h4 id = 'bag_2'>RFID Luggage Bag</h4>\n<br>\n<p>The luggage bag was the second RFID-enabled bag we built. It was similar to the backpack, but featured more of a 'check-in/check-out' model for the RFID items rather than the persistent reads we featured with the backpack. The luggage bag has a single RFID reader whose position is embroidered on the inside of the bag. The luggage piece also has four LEDs embedded in the handle of the bag. Again, the LEDs use green and red to convey the presence of an item in the luggage case.</p>\n\n<p>The luggage piece also has one additional feature that is new from the backpack version: a magnetic-snap toiletry case. The magnetic snaps used to hold the bag in place serve the dual purpose of also acting as contacts through which we can determine the presence of the toiletry bag. </p>\n\n<div class='image_block'>\n\t<img width='600px' src='/images/backpack_imgs/luggage_01.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/luggage_02.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/luggage_03.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/luggage_04.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/luggage_05.JPG' alt=''/>\n</div>\n\n<div class='video_block'>\n\t<iframe width='600' height='338' src='http://www.youtube.com/embed/9ouv_HWexrs' frameborder='0' allowfullscreen></iframe>\n</div>\n\n<br>\n<h4 id = 'bag_3'>Fridge-light pack</h4>\n<br>\n<p>The fridge-light pack is a simple bag that has a row of LEDs embedded on the inside that turn on when the bag is opened and off when the bag is closed. To achieve this functionality, we put a magnet on the inside back of the zipper and sewed a magnetic switch at the base of the zipper path. When the zipper comes close enough to the magnetic switch, the lights turn off. Again, all electronics for this bag are held within a side pocket of the bag and are out of the way.</p>\n\n<div class='image_block'>\n\t<img width='600px' src='/images/backpack_imgs/fridge_01.JPG' alt=''/>\n</div>\n\n<div class='video_block'>\n\t<iframe width='600' height='450' src='http://www.youtube.com/embed/o3_sfi4JCYg' frameborder='0' allowfullscreen></iframe>\n</div>\n\n\n<br>\n<h4 id = 'bag_4'>Two Friendship packs</h4>\n<br>\n<p>The friendship packs are a pair of bags who react to the presence of one another. The packs have an embedded electroluminescent panel placed behind a cut pattern in the front pocket of the bag. When the bags approach each other, the front panel begins to light up. At a medium range, the panel will begin to fade in and out, with the approximate rate of a slow breathing pattern. As the bags get closer and are within close range of each other, the pattern speeds up to that of a racing heart-beat. The idea being that the bags are excited to 'see' each other and react as old friends would when reacquainting.</p>\n\n<p>Each bag has a zigbee module inside that can read the relative strength of the other module's sent signal. As the bags are spread apart, the radio signal between the two becomes weaker, and can be loosely correlated to distance. </p>\n\n<div class='image_block'>\n\t<img width='600px' src='/images/backpack_imgs/friend_01.JPG' alt=''/>\n\t<img width='600px' src='/images/backpack_imgs/friend_02.JPG' alt=''/>\n</div>\n\n<div class='video_block'>\n\t<iframe width='600' height='450' src='http://www.youtube.com/embed/HPnrn62rhNQ' frameborder='0' allowfullscreen></iframe>\n</div>\n\n<br>\n<h4 id = 'bag_5'>Ambient-light pack</h4>\n<br>\n<p>The ambient light pack is a quick bag that has embedded light-sensors on it's sides. As the bag enters a darker environment the bag lights - a digital glow-in-the-dark bag. </p>\n\n<div class='video_block'>\n\t<iframe width='600' height='450' src='http://www.youtube.com/embed/Rj-5TYcM0Dk' frameborder='0' allowfullscreen></iframe>\n</div>\n\n<br>\n<h4 id = 'bag_6'>Camera pack</h4>\n<br>\n<p>The camera pack was an adaptation of the trip-tracker pack originally proposed. The bag has two embedded cameras that are placed in the front strap and back panel of the bag. The cameras stick out slightly from the material and are set to take a picture every 30 seconds. The pictures are stored internally on a micro-SD card that can be removed later to upload the pictures to a computer or phone. We didn't have a chance to finish the construction of the bag during the trip, but the hardware was completed and ready to be integrated. </p> \n\n \n\n<div class='image_block'>\n\t<img width='600px' src='/images/backpack_imgs/total_01.JPG' alt=''/>\n</div>\n\n</div>", 
            "largePic": "nav/jansport_lrg.jpg", 
            "order": 21, 
            "slug": "backpack", 
            "smallPic": "nav/jansport.jpg", 
            "summary": "summary", 
            "title": "backpackHacks", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>For part of my thesis work, I needed to design a (near) real-time feedback loop between the sensors in a mobile phone and a browser-based 3D rendering environment. Additionally, I needed an architecture that would facilitate this across any number of browser screens and any number of simultanesouly-active mobile devices. Having built this architecture (using Tornado and Three.JS), I realized it had the potential for a great game environment. Ad-Hoc multiplayer games could be created where the display is any browser window and everyone already has a controller: their phone. While my thesis focuses mostly on camera-based interactions, the idea of using other sensors as a game controller was partially inspired by a quick tech demo by Justin Gitlin. </p>\n\n<p>The full featured library and game that I plan to develop is currently in slow-development as my thesis work takes priority, but some early code can be found on my <a href='https://github.com/isTravis/mobileSockets' target='blank'>GitHub repo</a>. </p>\n\n<iframe width='600' height='338' src='http://www.youtube.com/embed/RU9F1cVD3_E' frameborder='0' allowfullscreen></iframe>\n\n<iframe width='600' height='450' src='http://www.youtube.com/embed/VyzZEgsGZ1o' frameborder='0' allowfullscreen></iframe>", 
            "largePic": "nav/mobileSocket_lrg.jpg", 
            "order": 13, 
            "slug": "mobilesockets", 
            "smallPic": "nav/mobileSocket.jpg", 
            "summary": "blank", 
            "title": "mobileSockets", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>Shubble is your sharing bubble. The project aims to remove the disparity between human-to-human knowledge sharing and computer-to-computer knowledge sharing. The project seeks to leverage the same proximity-based privileging that humans employ in daily conversation. Face to face conversation leverages the shared physical location of its members, whereas digital conversation leverages cloud-based permissioning (accounts, passwords, etc). By creating a shared digital space based on device location, we create a seamless way to share digital les with those in your meeting space. The project identifies mobile devices in a common space and creates a shared repository between the owners of those devices.</p>\n\n<img src='/images/project_pics/shubble_01.jpg' width='600px'/>\n\n<p>The project uses a knocking signal to identify proximal devices and to initiate a shared data space. Shubble creates a persistent shared space for users to continue and enhance their conversation, wrapping around existing cloud services such as Google Docs or Dropbox.</p>\n\n<img src='/images/project_pics/shubble_02.jpg' width='600px'/>\n\n<p>Shubble was initially developed in collaboration with <a href='mailto:asugaya@mit.edu' target='blank'>Andrew Sugaya</a> and <a href='mailto:jpchan@mit.edu'>Jenny Chan</a>. <a href='mailto:sjsu@mit.edu' target='blank'>Stephanie Su</a> made contributions to the project later in development.</p>\n\n<iframe src='//player.vimeo.com/video/76385697' width='600' height='338' frameborder='0' webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>", 
            "largePic": "nav/shubble_lrg.png", 
            "order": 10, 
            "slug": "shubble", 
            "smallPic": "nav/shubble.jpeg", 
            "summary": "Knock to connect", 
            "title": "shubble", 
            "z_subProjects": []
        }, 
        {
            "html": "<h3 id='introduction'> Introduction </h3>\n<br/>\n\t<p> Project Surkat is an effort to explore hardware design tools with an eye critically focused on what behaviors and techniques are simply historical artifacts and thus, can be greatly improved to match our current design ecosystem. We are no longer stuck with hardware design tools being restricted to multimeters and oscilloscopes. Unfortunately, most computational hardware design tools simply try to reproduce the functionality and experience of these bench-top devices. Project Surkat looks to identify and propose alternative tools for hardware design, simulation, sharing, and fabrication leveraging modern tools and techniques. Work on Project Surkat has been completed as part of MAS.890 Independent Study under the guidance of Professors Leah Buechley and Mitch Resnick.</p>\n\n<h3>Table of Contents </h3>\n\n<div id='tableOfContents'>\n<ol id='tableOfContents'>\n\t<a href='#introduction'><li>Introduction</li></a>\n\t<a href='#motivation'><li>Motivation</li></a>\n\t<a href='#design_goals'> <li>Design Goals</li> </a>\n\t<a href='#existing_work'> <li>Existing Work</li> </a>\n\t<a href='#dynamic_circuit_design'> <li>Dyanmic Circuit Design</li> </a>\n\t<a href='#dynamic_circuit_simulation'> <li>Dynamic Circuit Simulation</li> </a>\n\t<a href='#user-centric_design'> <li>User-centric Design</li> </a>\n\t<a href='#clipping_workbench'> <li>Clipping Workbench</li> </a>\n\t<a href='#search'> <li>Search</li> </a>\n\t<a href='#sharing_and_collaboration'> <li>Sharing and Collaboration</li> </a>\n\t<a href='#wiki-style_documentation'> <li>Wiki-style Documentation </li> </a>\n\t<a href='#commenting'> <li>Commenting</li> </a>\n\t<a href='#version_control'> <li>Version Control</li> </a>\n\t<a href='#future_visions'> <li>Future Visions</li> </a>\n\t<a href='#source_code'> <li>Source Code</li> </a>\n</ol>\n</div>\n\n<h3 id='motivation'> Motivation </h3>\n<br/>\n\t<p>The motivation for this project was initially spurred by the difficulty faced when simply trying to get started with hardware simulation. Creating an environment where a user of minimal (or even advanced) experience can jump on any arbitrary computer and begin working is extremely time consuming and inefficient. Furthermore, the experience can vary strongly for circuit simulations from device to device as local processing power is a direct constraint. </p>\n\n\t<p> The open software movement has led to unprecedented growth worldwide of industries from communications to entertainment. Furthermore, open software has enabled millions of people to learn and become passionate about software design and engineering. Programming is no longer the domain of the college-educated computer science major - but of all people. Software programming can now be thought of as analogous to the quest for worldwide literacy spurred by construction of libraries. However, the domain of hardware design has not been able to make the same strides as open software. Nonetheless, there is still a growing open hardware movement that indicates the same excitement and opportunity that came with open software can exist for hardware. However, hardware deals with atoms, rather than simply bits, so the distribution and construction have not reached the same zero-cost that is enjoyed in the software world. That said, the price is decreasing and the progression of open hardware can be accelerated by developing the right tools to enable people of all backgrounds and experiences to learn and play with hardware.</p>\n\n\t<p>Key problems that exist in the current hardware design ecosystem include 1) A large barrier to entry to both tools and knowledge, 2) specialized tools that do not match current cultural design norms (hardware design/simulation tools feel very foreign), and 3) disperse and non-uniform documentation. The large barrier to entry is due to the price of most software tools, the hardware requirements to run the tools, the experience needed to setup a development environment, and the experience to understand the functionality of the tools. Furthermore, hardware tools tend to exist in a world of their own. They have novel distribution and installation procedures, unconventional library and parts database management systems, and unconventional reference or help documentation. This creates a situation in which designers and engineers cannot leverage their previously learned skills when designing hardware. Lastly, while there are several efforts (e.g. <a href='http://octopart.com/'>Octopart</a>, <a href='http://upverter.com/'>Upverter</a>, etc) to create a unified resource for hardware documentation, the ecosystem is still dominantly one in which different manufacturers use their own standards to distribute datasheets, technical specifications, part simulation files, and footprint layouts. This fragmentation is likely both a technical and business-model challenge, yet in identifying potential tools, I feel some progress can be made.</p>\n\n\t<p>There also exist several aspects of the open-software movement that provide great value and virality that the open hardware movement has yet to adopt. Features such as the ability to easily copy-and-paste bits of code around for arbitrary applications, to contribute to debugging a shared project, and to efficiently search for existing work are ways that open-software provides enormous value - and I think similar features would be equally valuable for open-hardware.</p>\n\n\t<p>\tLastly, hardware tends to be restricted to the domain of academics who have had some amount of formal training. As seen in the open-software movement, by creating a low barrier to entry one enables worldwide learning of a discipline that is exceedingly complex and intricate. By contributing to large open-source software projects (e.g. Linux), novice developers are able to work with and understand enormously complex software structures and architectures, and through this experience, become an advanced programmer. Such an opportunity simply does not exist in the realm of hardware development.</p>\n\n\n<h3 id='design_goals'>Design Goals</h3>\n<br/>\n\t<p> The design focus for the project is centered on two main principals: immediate feedback and open-access. Much of the inspiration for the immediate feedback comes from <a href='http://worrydream.com/'>Bret Victor</a>, who argues that it is imperative designers have immediate feedback between their 'canvas' and 'brush'. To understand and refine their actions, an immediate response to their movements and efforts must be seen. The open access goal comes from years of frustration of dealing with proprietary circuit schematic file types and layouts that aren't cross-platform compatible. With the underlying goal of enabling all people to design and understand hardware, creating an open medium that is not constrained to a single design tool is necessary. </p>\n\n\t<p>To address the accessibility, I have made the decision to explore browser-based circuit capture, editing, and simulation. By using the browser as a common platform, we enable access across a multitude of computing devices and standardize the environment in a language that is very well understood by many. Furthermore, a browser-based tool allows us a distinct set of resources (i.e. the web) and the opportunity to remove the step of local installation. No setup is required to begin working in a browser-based environment. </p>\n\n\n<h3 id='existing_work'>Existing Work</h3>\n<br/>\n\tTraditional open hardware tools and references tend to break into 4 categories: reference banks, demonstrations, desktop software packages, project blogs.\n\n<table id='priorArtTable' style='background-color: fff;' width='600px' border='1' cellspacing='0' cellpadding='0' text-align='center'>\n\t<tbody>\n\t\t    <!-- Results table headers --></p>\n\t\t<tr>\n\t\t<th>Project</th>\n\t\t<th>Desktop or Browser</th>\n\t\t<th>Schematic Capture</th>\n\t\t<th>PCB Capture</th>\n\t\t<th>Simulation</th>\n\t\t<th>Reference Tool</th>\n\t\t<th>Project Blog</th>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://www.falstad.com/circuit/'>Falstad Apps</a></td>\n\t\t<td>Browser</td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' text-align='center'/></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://dangerousprototypes.com/'>Dangerous Prototypes</a></td>\n\t\t<td>Browser</td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://www.opencircuits.com/Main_Page'>Open Circuits</a></td>\n\t\t<td>Browser</td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://sourceforge.net/apps/mediawiki/tinycad/index.php?title=TinyCAD'>TinyCad</a></td>\n\t\t<td>Desktop</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://www.cs.york.ac.uk/jbb'>Java Breadboard</a></td>\n\t\t<td>Desktop</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://qucs.sourceforge.net'>Quite Universal Circuit Simulator</a></td>\n\t\t<td>Desktop</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://fritzing.org/'>Fritzing</a></td>\n\t\t<td>Desktop (some Browser based social aspects)</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://www.designspark.com/pcb'>Design Spark</a></td>\n\t\t<td>Desktop</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://kicad.sourceforge.net/'>KiCAD</a></td>\n\t\t<td>Desktop</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://garlicsim.org/'>GarlicSim</a></td>\n\t\t<td>Desktop</td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://www.linear.com/designtools/software/'>LTSpice</a></td>\n\t\t<td>Desktop</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://cadsoftusa.com/'>Eagle</a></td>\n\t\t<td>Desktop</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://www.geda-project.org/'>gEDA</a></td>\n\t\t<td>Desktop</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://www.openhardwarehub.com/'>Open Hardware Hub</a></td>\n\t\t<td>Browser</td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://upverter.com/'>Upverter</a></td>\n\t\t<td>Browser</td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td><a href='http://www.thingiverse.com/'>Thingiverse</a></td>\n\t\t<td>Browser</td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td></td>\n\t\t<td><img id='check' src='http://i.imgur.com/JGDCl.png' /></td>\n\t\t</tr>\n\t</tbody>\n</table>\n<br/>\n\n\t<p>Since the beginning of this project in January of 2012, a few startups have been launched that solve my initial intent of designing a browser-based circuit capture tool. Thus, the focus has moved from crating a simple browser based circuit editor (and simulator) to identifying more dynamic features that could enhance learning, design, collaboration, inspiration, and lower costs. The two most prominent startup efforts are <a href='http://upverter.com/'>Upverter</a> - who enable many fantastic tools such as browser-based editing, git-like sharing of projects, and centralized parts databases - and <a href='https://www.circuitlab.com/'>CircuitLab</a>. CircuitLab has taken an additional (and very important) step of enabling browser-based circuit simulation. The CircuitLab product is a fantastic step in the right direction, however, it is still constrained to the standard 'benchtop replication'. That is, there is one window for circuit design and a separate pop-up window that appears every time a simulation is run. The simulation and circuit capture tools do not yet take advantage of all the opportunities available given the online, browser-based environment it lives in (real-time simulations, dynamic circuit annotations, etc). </p>\n\n\t<p>An open-source browser-based tool has also been created for the <a href='http://mitx.mit.edu/'>MITx</a> program. This nifty little tool is contained in two simple javascript files and provides browser based circuit capture and simulation (<a href='https://github.com/zupolgec/circuit-simulator'>source</a>). The tool is certainly a work in progress, and doesn't have all the bells and whistles of the other two startups, but - of enormous value - it's open and free. Furthermore, it's simplicity makes it easy to edit and use as a baseline for mocking up circuit design tools (as I do for many of the efforts below). </p>\n\n<h3 id='dynamic_circuit_design'>Dyanmic Circuit Design</h3>\n<br/>\n\t<p>One of the fundamental goals of the project is to investigate ways of making the circuit capture environment more dynamic. That is, leveraging the resources available to modern computers to move beyond the analogy of a static circuit drawings on a piece of paper. Enabling a dynamic environment that can provide meta-data when needed, can zoom to different levels of detail, or can be simultaneously edited by multiple people, opens exciting opportunities for the future of hardware design. </p>\n\n\t<h4>Copy and Paste is Key</h4>\n\t\t<p>One of the most important notions of the software is the ability to copy and paste lines of code from one application to another. This not only enables one to leverage past applications that have been created, but enables people to leverage the algorithms and structures that were used for that application. Even if the end goals are different, it is very likely someone has used the mechanisms and methods you need for your code. This serves both as an efficiency tool and a learning tool. Extremely complicated applications can be crafted and slowly understood by a novice that may not have been able to write the code from scratch.</p>\n\n\t\t<p>Such functionality does not conveniently exist for hardware. So far, many open source hardware projects tend to be all-or-nothing. That is, their source files and data are public, but it is rather difficult to do anything besides reproduce that exact piece of hardware. Copying a small bit of the hardware design and mashing it together with small bits from three other designs is not something that can be easily done. Thus, I claim that designing a method to easy copy and paste small bits of circuitry between projects is extremely important. I'll explore a potential implementation of this in the <a href='#clipping_workbench'>Clipping Workbench</a> section. </p> \n\n\n\t<h4 id='circuit_segmentation'>Circuit Segmentation</h4>\n\t\t<p>Another area that a dynamic browser-based editor can benefit the hardware design workflow is in circuit segmentation. Efficiently navigating a large, complex circuit design can be a very difficult task, especially if it is someone else's design. In software, the 'Find' command serves as one tool to tackle this same problem. However, since we hardware design isn't by demand text based, we can't leverage this same solution. Furthermore, to make browsing easier, there are other nice little text-editor features that have been implemented in the realm of software, such as collapsing certain blocks of code (see picture below).</p>\n\n\t\t<center><img src='http://i.imgur.com/hI8V5.png' alt='collapsable text editor'/></center>\n\n\t\t<p>Thus, we need to be more clever in how we approach machine parsing and segmentation of hardware design circuits to enable more fluid navigation through complex circuit diagrams. One possible way of doing this is to map the hierarchies that exist in the designer's mind to the circuit editor.</p>\n\t\t<center><img src='http://i.imgur.com/zjieh.png' alt='Heirarchical segmentation'/></center>\n\t\t<p>Each block could be annotated with a name that is searchable, and can be defined by the designer. This also feeds into the previous idea of copy and paste. It creates a natural method by which to select the size of a chunk of circuit to copy. It is likely that the key structure as identified by the designer is key because of it's completeness. It is likely a designer would block off a 'lowpass filter', but unlikely a designer would bock of '3/4 of a lowpass filter'. To borrow the collapse arrow notion from software, each sub-box could be hidden to show only a high-level black box or all the low level details.</p>\n\t\t<center><img src='http://i.imgur.com/TGIsu.png' alt='Collapsable Heirarchical segmentation'/></center>\n\n\t<h4>Alternative Input</h4>\n\t\t<p>Software always exists in one medium - text. So copying and pasting is straightforward. But many circuit diagrams are roughly sketched or exist as images online or in textbooks. Enabling a system that can take a picture and extract the meaningful circuit elements (or at least best guess) may help bridge the gap between the multi-modal reality of circuit design and the simplicity of a single editing system. This would obviously be a somewhat lossy process in that computer vision algorithms would likely have trouble identifying specific ICs or op amps. However, perhaps a mixed-mode process would still greatly increase the ease of porting a hand drawing to a digital system. That is, a language of general parts could be used and post-processing input of device numbers, values, etc could complete the process.</p>\n\t\t<center><img id='no-border'src='http://i.imgur.com/OL0Bc.png' alt='Circuit capture through optical recognition'/ width ='600px'></center>\n\n\n<h3 id='dynamic_circuit_simulation'>Dynamic Circuit Simulation</h3>\n<br/>\n\t<p>Circuit simulation is a realm that is still nearly identical (if not less capable) than what designers are accustomed to with physical oscilloscopes. The general flow of circuit simulation as it currently exists begins with a circuit schematic, test points are marked for either current or voltage monitoring, and then a pop-up window with a graph depicting the temporal or frequency plots of the circuit's behavior are displayed. This is analogous to a digital oscilloscope popping up on screen, with the added complication that you can't tweak a circuit schematic in real-time to view how the output changes dynamically. It is nearly always a serial process: tweak schematic, view simulation, tweak schematic, view simulation...</p>\n\n\t<p>There is no technical reason for our circuit simulation to be stuck in this domain. A parallel solution is possible, and I argue, is key for developing an intuitive understanding of how circuits behave. The immediate feedback, as stressed by Bret Victor (<a href='https://vimeo.com/36579366'>hear him explain his ideology here</a>), is key to the design process. Beyond exploring real-time simulations, we can also explore different approaches to displaying the simulation data. Pop-up windows tend to break the flow of the circuit, and by design, make the simulation environment feel like a completely separate world. In-schematic simulations are one option, as are dynamic calculators in the sketching environment. </p>\n\t\n\n\t<h4>In-schematic Simulations</h4>\n\t\tBret Victor has a great demonstration of in-schematic simulations (image below) and shows how we can gain a more intuitive, fluid understanding of how a circuit behaves by placing schematic and behavior on a single view. Bret provides some great javascript tools for embedding html documents with real-time controls and variables (<a href='http://worrydream.com/Tangle/'>Tangle</a>). Though his circuit demonstration does not have source-code documentation, it is easy to see that Tangle serves as the backend to this project. \n\t\t<center><img id='no-border' src='http://i.imgur.com/GLt9Z.png' alt='In-schematic simulation'/></center>\n\n\t<h4>Dynamic Calculators</h4>\n\t\t<p>Dynamic calculators are another potential opportunity to help hardware designers quickly and easily test behavior of common circuits. The calculators can be easily crafted from elements within the current circuit schematic, but can be persistent in the design environment across all projects. This way, a user could make a quick 'low pass filter calculator' that is always available and be quickly drawn upon to make some quick design decisions. The calculators act as persistent simulations of simple elements that can be drawn upon at anytime. Further leveraging the browser-based aspect of the design, calculators can be shared, forked, and tweaked. Somewhat analogous to Github 'gists', these calculators are essentially quick lines of code with very specific functionality that can be applicable to many larger circuit designs. A quick mockup of dynamic calculators can be found here: <a href='http://web.media.mit.edu/~trich/code_savedCalc/'>demo</a> | <a href='http://web.media.mit.edu/~trich/code_savedCalc/code_savedCalc.zip'>source</a> </p> \n\n\t\t<a href='http://web.media.mit.edu/~trich/code_savedCalc/'><img id='no-border' src='http://i.imgur.com/xXg0j.png' alt='Dynamic calculator mockup' width ='600px'/></a>\n\n<h3 id='user-centric_design'>User-centric Design</h3>\n<br/>\n\t<p>One of the most important decisions in designing an open hardware design tool is deciding on what the target audience will be. Design choices must be made that reflect whether the tool is for all audiences and is completely general, or niche audiences and is tailored to the specific needs of that audience. Is the tool for novice designers? low-level hardware experts? digital hardware designers? students? educators? etc... Since I'm not actively striving to launch a platform and foster a community around the tool, I tend to lean towards the completely general, all-audiences tool in my design thinking. If I were to launch an actual platform with intentions of fostering a community around it, this decision would likely be foolish in that it would add extreme complexity to my task and would keep me from focusing on a appropriately sized goal. Many similar community-based tools start with a small niche audience and scale if and only if the conditions are right for introduction to a large general audience. It is much more difficult to launch a product that is completely general and try to reach a critical threshold in community.</p>\n\n\t<p>Given that, one of the major areas of user-differentiation is in user experience. Hardware design tools often have a steep initial learning curve that can cause a great barrier to entry. There are many subtle layers to fully understanding how a circuit is behaving, and often these layers can become an overwhelming amount to internalize. The same can be said for the multitude of features, parts libraries, simulation types, circuit capture options, and design assumptions that are part of circuit design tools. Again, while many of these options are necessary and helpful for advanced users, they can be a burden to novices. The seemingly-obvious answer to this challenge is to have a design tool that adapts to a users skill level and either hides or limits the numbers of options and variables available at any one time. Essentially, creating an 'easy', medium', and 'hard' mode for circuit design. However, we must also be careful to acknowledge that people can be either poor at self-diagnosing their ability or resistive to changing their diagnosis. That is, as a designer becomes more talented, they may not 'promote' themselves to the 'medium' difficult tool thinking that they are still a novice. This can slow advancement and in the end act as another barrier to access. Furthermore, much of learning is done through trial and error of options and tools a user does not fully understand. Thus, simply hiding options may hinder the learning process of a user. People are simply to varied in their learning styles to make an assumption inherent in the tool about what features are good for novices versus advanced users. So when designing a tool to match the varied skill level of designers, it is important to carefully consider the trade offs of making a tool 'more available'. </p>\n\n\t<p>Perhaps rather than hiding options to novices, we make certain options available that will likely only be used by novices. Such options include toggling help bubbles, component vs. symbol view (see mockup picture below), or enabling high-level block-by-block construction of circuits (e.g. letting users copy and paste circuit sections as entire black backs a la the circuit segmentation idea above). Other aspects of the experience could also be changed, such as how part navigation occurs (high-level first or low-level first) or the depth of assumptions made by the simulator.</p>\n\t<img src='http://i.imgur.com/Z8UNx.png' alt='Component vs symbol view' width='600px'/>\n\n\n\t<p>Slightly easier that solving for differences in difficulty, is solving for differences in conventions. A simple toggle can be used to switch between common European symbols and common American symbols. The toggle could also be created to include an option for custom symbols and real-view symbols (i.e. what that element really looks like).</p>\n\t<center><img src='http://i.imgur.com/UaTpf.png' alt='European vs. American view'/></center>\n\n\n\n<h3 id='clipping_workbench'>Clipping Workbench</h3>\n<br/>\n\t<p>As mentioned earlier, in the <a href='#dynamic_circuit_simulation'>Dynamic Circuit Design</a> section, it is key to have a fluid copy and paste tool for hardware design. The ability to copy and paste bits of code from one source to another serves as a key learning and development feature of the open software movement. Software has benefited from having copy and paste functionality universal across nearly all computing platforms, so it is natural and easy to move text from one source to another. However, once you move to the domain of hardware design, there is no standard mechanism for moving schematic elements from one source to another. This is a problem across design ecosystems and even within the same ecosystem in certain cases. </p>\n\n\t<p>The MITx schematic capture code provides a nice copy and paste feature that relies on a JSON representation of circuit elements. I have modified this code to implement a 'Clipping Workbench'. The simple idea is that there exits a drawer of previous clips (analogous to your computer's clipboard) that stays with you across projects and schematics. This simple design allows analog algorithms, techniques and ideas to cross pollinate across designs a projects. It allows users to take bits from many different projects and reassemble them into a single circuit with entirely different functionality. This is a common practice in software design, and very difficult in hardware design. A demo of and source code for this mockup can be found here: <a href='http://web.media.mit.edu/~trich/code_surkat/'>demo</a> | <a href='https://github.com/isTravis/project_Surkat'>source</a></p>\n\n\t<a href='http://web.media.mit.edu/~trich/code_surkat/'><img src='http://i.imgur.com/t8mhV.png' alt='Clipping Workbench'/ width ='600px'></a>\n\n\n<h3 id='search'>Search</h3>\n<br/>\n\t<p>Another challenge facing hardware designers is search. In the world of software, text search is relatively easily to implement and straight forward. However, in the world of hardware design, amongst circuit symbols and visually annotated diagrams, how does one find what they need? Search is key to enabling an open hardware community. Without it, users can't stumble onto other designs or efficiently find help and advice from designs previously made. I see four main methods for search in hardware: 1)Search by comment, 2) search by part, 3) search by waveform, and 4) search by performance.</p>\n\n\t<p>Searching by the commenting and labels applied to a hardware design is most obvious, and frequently the easiest. As long as a user annotates, titles, and labels their design properly and with appropriate tags, it can usually be found with minimal effort. However, this technique can often be far too general. Say I'm searching for a low pass filter. The results I would get would be stand-alone low pass filter designs, and in all likelihood, the algorithm would miss the lowpass filter that was part of a much larger infrared camera circuit. Often, finding a result in context (i.e. how it connects to other circuit stages) can be more useful than the stand-alone circuit itself. Thus, in this case, the filter integrated into the camera circuit would likely be of more use to me, even though it doesn't match my search query text for 'low pass filter'.</p>\n\n\t<p>One attempt at solving this problem is to enable users to search by circuit parts. By searching for 'low pass filter', the results will return all circuits that have low pass filters in them as determined by either proper annotation (a la the circuit segmentation idea) or contain a configuration or IC that is known to act as a low pass filter. There are enormous databases of integrated lowpass filters, and by searching over these databases for part numbers and comparing them to various stored designs, I may be able to more efficiently find what I'm looking for.</p>\n\n\t<p>Furthermore, search by parts could enable the wandering designer to search based on components they currently have available to see what others have done with that part and perhaps gain some inspiration. By searching for 'LM741', one can see all the various ways in which that specific IC has been used and perhaps inspire new ideas for how to solve some given problem.</p>\n\n\t<p>Searching by waveform is where I think hardware search could really become useful. The general idea is that a circuit schematic is stored along with automatically generated simulations of the behavior of that circuit at various points and under various conditions. A user can then simply draw the general curve of the output they desire, and search for circuits who have a similar output function. For example, one could draw a simple low pass curve and the system would return filters of all different configurations.</p>\n\t<img src='http://i.imgur.com/Wwfwj.png' alt='Search by waveform' width ='600px'/>\n\n\t<p>Alternatively, the user could use a generated simulation curve and search for similar waveform shapes at different parameters, e.g. give me a filter that has a cutoff like this but at high frequencies. In this way, the user is searching based on the functionality of the circuit rather than the constituent parts of the circuit in a way that doesn't require manual input from the original designer.</p>\n\n\t<p>Lastly, a parameter-based search could help users find circuits based on certain properties of the design. Properties such as the frequency range of the chips it uses, the power voltages used, 3dB point, the maximum capacitance of a system, etc. This may be a bit cumbersome to provide all possible options, but perhaps a clever user interface could fix the burden of too many dimensions. </p>\n\n\n<h3 id='sharing_and_collaboration'>Sharing and Collaboration</h3>\n<br/>\n\t<p>Collaboration is an important aspect of hardware design. In the software world, many tools exist to allow users to independently develop bits of a larger program and then merge changes together. This, while sometimes complicated, is relatively straightforward because of the text-based nature of software. In hardware, such a merge could be nearly impossible to perform. One option to work around this is to provide a collaboration mechanism that never needs merging, such a real-time editing of a single schematic. Instead of two designers independently working on their own parts and then merging, a unified platform could be created where all changes made by one user are reflected in real-time to the other collaborator. This is similar to the Google Docs approach to collaboration. Such an environment could also be used as a teaching aid where novice users watch the behavior of more experienced users in designing circuits. Fortunately, <a href='http://www.upverter.com'>Upverter</a> has implemented features allowing users to collaborate on a schematic in real time, giving us a glimpse of what the future of hardware design may look like.</p>\n\t<img src='http://i.imgur.com/qwF8L.png' alt='Upverter collaboration' width ='600px'/>\n\n\n\t<p>The sharing ecosystem is of great importance for the open hardware movement. One key design choice I see is in building a system to be either user-focused or design focused. In the world of images, this distinction is analogous to the difference between Flickr and Imgur. Whereas Flickr puts the emphasis on the user (and people views that users photos), Imgur puts the emphasis on the image with breadcrumbs to the original author as a lesser detail. I argue in favor of a design-focused approach. Like Imgur, the open hardware platform should employ a system where designs are uploaded to a shared database and the only reference to the creator is a link saying 'created by'. This is in contrast to a system where, when viewing a file, you are essentially in the domain of the user, looking at one of their projects. The subtle difference implies different conventions of sharing. I think this subtle difference is important to maintain an open community of sharing where 'ownership' of a design is not used to diminish virality of a given work.</p>\n\n\n<h3 id='wiki-style_documentation'>Wiki-style Documentation</h3>\n<br/>\n\t<p>One of the benefits of using a browser-based implementation for a circuit design environment is that we can leverage off-site servers to store large databases of community-maintained information. This includes a central repository of parts, footprint files, datasheets, and ordering locations. This information can be added and maintained in a wiki-style manner where any user has the ability to edit and update the repository. This allows users to leverage the work of others in expediting their design time. It is currently very common practice to make your own part symbols, simulation files, and footprints in design tools such as PSPICe. Unfortunately, a lack of central repository makes it so that many users repeat the same effort every day because they are unable to leverage the work of their peers. A centralized wiki-style database of this information can serve as one solution. Again, Upverter has implemented this idea to a certain extent and the results are fantastic (image below). They have an intuitive interface for editing part details and a simple search tool to let users find what they need.</p>\n\t<img src='http://i.imgur.com/dEp3S.png' alt='Upverter parts' width ='600px'/>\n\t<p>They leverage a git model in that users can fork parts to create new iterations on-top of existing data. A centralized store of ordering information could also be used to enable one-click ordering of parts and hardware needed to construct a given circuit. This could greatly reduce the hassle that is typically experienced in scouring the internet for the parts you need to buy after you've designed your circuit. Furthermore, direct integration could allow users to be prompted when they choose an obsolete part for their project. The system could immediately offer alternatives that are readily available or less expensive. </p>\n\n\n<h3 id='commenting'>Commenting</h3>\n<br/>\n\t<p> Hardware  design has the unique challenge that it is constrained to spatial dimensions that make 'sense'. That is, at some point, you need to physically make your hardware design in reality, so if your circuit schematic is the length of four football fields, you're going to have trouble physically building that design. Thus, designs tend to be spatially small and intricate in their minute complexities rather than their length. Note that this is not the case for software. You could have 10 million lines of code and implement the program as easily as a 10-line program. The main take-away from this is that you can afford to be comfortably spaced in software design, but not in hardware design. Thus, efficiently commenting in hardware can be difficult as the general space you are working with is not linear as is the case with text. The simple in-line commenting procedures that people are accustomed to in software don't translate well to hardware. New tools for annotation and commenting must therefore be developed.</p>\n\n\t<p>In cases such as this, a layered commenting system is necessary. Comments must be available at the lowest level of the circuit and at the highest level of the circuit simultaneously even though these two things can exist in the same place. When I hover over a specific IC, does the user want information about that IC or the overall network of components around that IC that make it the centerpiece of a low pass filter? This matches with the <a href='#circuit_segmentation'>Circuit Segmentation</a> bits that were mentioned earlier. It should also be noted that we are not restricted to simple text-comments - quick sketches or images could also be useful. The zoom-ability of a circuit is important and can be used to make assumptions about what level of commenting is desired. If the whole circuit is in view, then the highest level comment is shown, whereas if a user is zoomed to only an IC on-screen, then the IC comment is displayed</p>\n\n\t<p>The workflow for creating comments in a system like this could be as follows: \n\t\t<ul>\n\t\t\t<li>Select components pertinent to a comment</li>\n\t\t\t<li>Create text/image/sketch comment</li>\n\t\t\t<li>A rectangle containing all elements selected is created with the comment source</li>\n\t\t\t<li>Hover over any of the originally selected elements (or the area in between them) to view, edit, or delete the comment. </li>\n\t\t</ul>\n\n\t\tThe rectangles displayed at any given time can be a function of the zoom-level of the circuit. Furthermore, note that this allows elements to be part of any number of comments and the existence of overlapping comments is allowed. \n\t</p>\n\n\n\n<h3 id='version_control'>Version Control</h3>\n<br/>\n\t<p>Version control in software is absolutely critical and makes large-scale development with many contributors possible. In hardware, version control is a more difficult process due to the non-text medium. Existing version control systems for hardware rely on image difference algorithms to identify what in the circuit has visually changed. This of course can miss enormous levels of detail such as changes to the component value or linked part information. By creating a unified browser-based environment, you can also enable a version control system that runs on the underlying software being used to generate the circuit schematic. In the MITx-based environment, this could be performed on the standardized JSON that defines a circuit diagram. The team at Upverter has implemented a system like this relying on a git-like backend. Their system has been implemented for both schematics and part definitions. Their work demonstrates a huge step forward in terms of enabling hardware design collaboration and speaks to the benefits of using a browser-based implementation.</p>\n\n\t<img src='http://i.imgur.com/yWkMy.png' alt='Upverter parts' width ='600px'/>\n\n\n<h3 id='future_visions'>Future Visions</h3>\n<br/>\n\t<p>The real excitement of an open-hardware movement isn't the opening of the hardware itself, but what this openness facilitates. There are truly exciting opportunities ranging from enabling education to providing customers with options to repair broken devices. The true beauty of open hardware, in my eyes, is more than simply enabling what has happened in software to happen in hardware. When designers have the ability to learn and access hardware knowledge as easily as they do software knowledge - yes, there will be the same benefits as experienced in the open software movement - but more importantly, we will have a world of people who now have access to both hardware and software knowledge. When this situation occurs we enable people to design both hardware and software in unity such that the two are perfectly matched to each other. As Alan Kay said, 'people who are really serious about software should make their own hardware'. Open hardware makes it possible.  </p>\n\n<h3 id='source_code'>Source Code</h3>\n<br/>\n\t<p><a href='https://github.com/isTravis/project_Surkat'>Source code for my Schematic Capture/Simulation/Workbench Fork</a> | <a href='http://web.media.mit.edu/~trich/code_surkat/'>Demo</a></p>\n\t<p><a href='https://github.com/zupolgec/circuit-simulator'>Source code for MITx schematic editor</a></p>\n\t<p><a href='http://web.media.mit.edu/~trich/code_savedCalc/code_savedCalc.zip'>Source code for dynamic calculators</a> | <a href='http://web.media.mit.edu/~trich/code_savedCalc/'>Demo</a></p>", 
            "largePic": "nav/surkat_lrg.jpg", 
            "order": 8, 
            "slug": "surkat", 
            "smallPic": "nav/surkat.jpeg", 
            "summary": "Browser-based circuit capture and simulation", 
            "title": "surkat", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>Boredom Busters is a project built during the 2013 Comedy Hack Day, produced by Cultivated Wit, and hosted at the MIT Media Lab. Our project built a moxo platform. The project uses galvanic skin response sensors to detect when a wearer is becoming bored, and delivers an appropriate intervention.</p>\n\n<iframe src='//player.vimeo.com/video/76378094' width='600' height='338' frameborder='0' webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n\n<p>The project was built by <a href='http://www.buildempathy.com/' target='blank'>Elliott Hedman</a>, Henry Holtzman, <a href='http://www.mattstempeck.com/' target='blank'>Matt Stempeck</a>, and myself.</p>", 
            "largePic": "nav/boredom_lrg.jpg", 
            "order": 18, 
            "slug": "boredom", 
            "smallPic": "nav/boredom.jpg", 
            "summary": "summary", 
            "title": "boredomBuster", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>This page captures my projects and work completed for <a href='http://web.media.mit.edu/~raskar/' target='blank'>Ramesh Raskar's</a> Fall 2011 <a href='http://cameraculture.media.mit.edu/Fall2011ComputationalCamera' target='blank'>MAS.531 Computational Camera and Photography</a> course. </p>\n\t\t\t<div id='tableOfContents'>\n\t\t\t\t<h4>Projects</h4>\n\t\t\t\t<ol list-style='non'>                                   \n\t\t\t\t\t<li><a href='/ccp/channelmixing'>Project 1 - channelMixing</a></li>\n\t\t\t\t\t<li><a href='/ccp/lightfield'>Project 2 - lightField Photography</a></li>\n\t\t\t\t\t<li><a href='/ccp/multiflash'>Project 3 - multiFlash Imaging</a></li>\n\t\t\t\t\t<li><a href='/ccp/multichromatic'>Project 4 - multiChromatic Flash</a></li>\n                                       <li><a href='/ccp/multichromatic_motion'>Final Project - multiChromatic Motion</a></li>\n\t\t\t\t</ol>\n\t\t\t</div>", 
            "largePic": "nav/ccp_lrg.jpg", 
            "order": 6, 
            "slug": "ccp", 
            "smallPic": "nav/ccp.jpeg", 
            "summary": "Taking cameras beyond image capture", 
            "title": "compCamera", 
            "z_subProjects": [
                {
                    "html": "<div id='tableOfContents'>\n\t\t\t\t<h4>Contents</h4>\n\t\t\t\t<ol>\n\t\t\t\t\t<li><a href='#ccp_p1_intro'>channelMixing Intro</a></li>\n\t\t\t\t\t<li><a href='#ccp_p1_setup'>Getting Started with Cinder</a></li>\n\t\t\t\t\t<li><a href='#ccp_p1_photos'>Taking the Photos</a></li>\n\t\t\t\t\t<li><a href='#ccp_p1_mixing'>Channel Mixing</a></li>\n\t\t\t\t\t<li><a href='#ccp_p1_results'>Results</a></li>\n\t\t\t\t\t<li><a href='#ccp_p1_source'>Source</a></li>\n\t\t\t\t</ol>\n\t\t\t</div>\n\t\t\t\n\t\t\t<div id='ccp_p1_intro'>\n\t\t\t\t<h4>channelMixing Intro</h4>\n\t\t\t\t\t<p>For the first project, we've been assigned to take multiple photos, each lit differently, and then relight the image by mixing and matching the color channels.</p>\n\t\t\t\t\t<p>A demo set is shown below:</p>\n\t\t\t\t\t<img src='/images/project_pics/ccp/p1_files/relight_demo.jpg' alt='Gargoyles!' width='600px' align='center'/>\n\t\t\t\t<p>In the remixed image, the left-lit image is used as the blue channel while the right-lit channel is used as the red channel. </p>\n\t\t\t</div>\n\t\t\t\n\t\t\t<div id='ccp_p1_setup'>\n\t\t\t\t<h4>Getting Started with Cinder</h4>\n\t\t\t\t\t<p>To do this myself, I've chosen to use the <a href='http://libcinder.org/'>Cinder</a> library. According to their website title, Cinder is 'the library for professional-quality creative coding in C++'. Whatever that means - it's a sleek, well-documented library that provides some pretty nice tools for what we're trying to do (some simple image processing).</p>\n\t\t\t\t\t<p>If you're using OSX and Xcode - <a href'http://libcinder.org/docs/welcome/MacNewProject.html'>setting up Cinder</a> is incredibly easy. What's more, Cinder also offers some nice <a href='http://libcinder.org/docs/v0.8.3/guide___images.html'>introduction to image processing</a> using the framework - lucky us!. </p>\n\t\t\t\t\t<p>To play around with this, the first quick function I wrote was one to isolate the individual RGB channels of an image. Since the task is relatively computationally easy, I've opted to write this bit of code such that it is easy for humans to read and comprehend. The expense of that is computational elegance - it's a bit brute force - but for this task, that's fine.</p>\t\n\t\t\t\t\t<pre class='code'>\nvoid isolateChannel (Surface *surface, char color){\nSurface::Iter iter = surface->getIter( getWindowBounds() );\n\t\n\t//If you want to isolate red channel, \n\t//set blue and green pixels to 0\n\tif (color == 'r'){\n\t\twhile( iter.line() ) {\n\t\t\twhile( iter.pixel() ) {\n\t\t\t\titer.b() = 0;\n\t\t\t\titer.g() = 0;\n\t\t\t}\n\t\t}\n\t}\n\t//If you want to isolate green channel, \n\t//set blue and red pixels to 0\n\tif (color == 'g'){\n\t\twhile( iter.line() ) {\n\t\t\twhile( iter.pixel() ) {\n\t\t\t\titer.b() = 0;\n\t\t\t\titer.r() = 0;\n\t\t\t}\n\t\t}\n\t}\n\t//If you want to isolate blue channel, \n\t//set red and green pixels to 0\n\tif (color == 'b'){\n\t\twhile( iter.line() ) {\n\t\t\twhile( iter.pixel() ) {\n\t\t\t\titer.r() = 0;\n\t\t\t\titer.g() = 0;\n\t\t\t}\n\t\t}\n\t}\n}\t</pre>\n\t\t\t\t\t<p>The results, applied to our hard working Lego buddies, are as follows:</p>\n\t\t\t\t\t<img src='/images/project_pics/ccp/p1_files/lego_demo.jpg' alt='Lego!' width='600px' align='center'/>\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t</div>\n\t\t\t\n\t\t\t<div id='ccp_p1_photos'>\n\t\t\t\t<h4>Taking the Photos</h4>\n\t\t\t\t\t<p>Since we can't rely on Lego stock images forever, lets take our own. I've taken three sets of pictures, each lit from three different directions. Many thanks to the Viral Spaces group for providing the random toys and setup materials, and of course to Sam Luescher for use of his camera. </p>\n\t\t\t\t\n\t\t\t\t<p>For my light source, I used a goose-neck lamp placed behind the camera. To switch the light direction, I simply bent the lamp to one side or the other.</p>\n\t\t\t\t<p>Scene 1:</p>\n\t\t\t\t<img src='/images/project_pics/ccp/p1_files/scene1.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t\t<p>Scene 2:</p>\n\t\t\t\t<img src='/images/project_pics/ccp/p1_files/scene2.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t\t<p>Scene 3:</p>\n\t\t\t\t<img src='/images/project_pics/ccp/p1_files/scene3.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t</div>\n\t\t\t\n\t\t\t<div id='ccp_p1_mixing'>\n\t\t\t\t<h4>Channel Mixing</h4>\n\t\t\t\t\t<p>The goal at hand now is to take the three pictures in a given set, isolate one of their RGB channels, and then remix the three individual channels into a single image. There are four main functions that are used in my Cinder app to achieve this:</p>\n\t\t\t\t\t<ol>\n\t\t\t\t\t\t<li>setup</li>\n\t\t\t\t\t\t<li>processImage</li>\n\t\t\t\t\t\t<li>isolateChannel</li>\n\t\t\t\t\t\t<li>mergeChannels</li>\n\t\t\t\t\t</ol>\n\t\t\t\t\t<p>The setup function is used to import image resources and make function call to begin merge process. Handles the format conversion once the images are merged (simply converts the Surface objects to a Texture than can be drawn to screen)</p>\n\t\t\t\t\t<pre class='code'>\nvoid SurfaceBasicApp::setup(){\n\tSurface mySurface1 (loadImage( loadResource( 'scene1_a.jpeg' ) ) ); \n\tSurface mySurface2 (loadImage( loadResource( 'scene1_b.jpeg' ) ) );\n\tSurface mySurface3 (loadImage( loadResource( 'scene1_c.jpeg' ) ) );\n\t\n\tSurface processedImage( processImage(mySurface1, mySurface2, mySurface3));\n\tmProcessedImageTex = gl::Texture( processedImage ); //Convert the Surface object to a Texture object for drawing\n\t\n\timageWritten = 0;\n}\t\t\t</pre>\n\t\t\t\t\t\n\t\t\t\t\t<p>The processImage function calls the isolateChannel function for each image and subsequently calls the mergeChannels function. processImage returns the Surface object of the merged image.</p>\n\t\t\t\t\t<pre class='code'>\nSurface processImage( const Surface input1, const Surface input2, const Surface input3  ){\n\t// make the result be a copy of input\n\tSurface resultSurface1( input1.clone() );\n\tisolateChannel( &resultSurface1, 'r' );\n\t\n\tSurface resultSurface2( input2.clone() );\n\tisolateChannel( &resultSurface2, 'g' );\n\t\n\tSurface resultSurface3( input3.clone() );\n\tisolateChannel( &resultSurface3, 'b' );\n\t\n\tSurface resultSurface (mergeChannels(&resultSurface1, &resultSurface2, &resultSurface3));\n\t//Surface resultSurfaceFinal = resultSurfaceFinal;\n\t\n\treturn resultSurface;\n}\t\t\t\t\t</pre>\n\t\t\t\t\t\n\t\t\t\t\t<p>The isolateChannel function takes a pointer to a Surface object and char as input. The char specifies which channel will be isolated 'r', 'g', or 'b'. Since a Surface pointer is given, the Surface itself is edited.</p>\n\t\t\t\t\t<pre class='code'>\n// Takes input surface and isolates either an RGB channel. char color should be 'r', 'g', or 'b'. \nvoid isolateChannel (Surface *surface, char color){\n\tSurface::Iter iter = surface->getIter( getWindowBounds() );\n\t\n\t//If you want to isolate red channel, set blue and green pixels to 0\n\tif (color == 'r'){\n\t\twhile( iter.line() ) {\n\t\t\twhile( iter.pixel() ) {\n\t\t\t\titer.b() = 0;\n\t\t\t\titer.g() = 0;\n\t\t\t}\n\t\t}\n\t}\n\t//If you want to isolate green channel, set blue and red pixels to 0\n\tif (color == 'g'){\n\t\twhile( iter.line() ) {\n\t\t\twhile( iter.pixel() ) {\n\t\t\t\titer.b() = 0;\n\t\t\t\titer.r() = 0;\n\t\t\t}\n\t\t}\n\t}\n\t//If you want to isolate blue channel, set red and green pixels to 0\n\tif (color == 'b'){\n\t\twhile( iter.line() ) {\n\t\t\twhile( iter.pixel() ) {\n\t\t\t\titer.r() = 0;\n\t\t\t\titer.g() = 0;\n\t\t\t}\n\t\t}\n\t}\n}\t\t\t\t\t</pre>\n\t\t\t\t\t\n\t\t\t\t\t<p>The mergeChannels function takes as input the three isolated RGB channels. It creates a new Surface object called resultSurface that will eventually become our final image. Two simple while loops are used to iterate through each line, and then each pixel, of the images. At each iteration, the RGB values from each individual channel are assigned to the resultSurface object. Once the entire result image has been composed, the Surface object is returned.</p>\n\t\t\t\t\t\n\t\t\t\t\t<pre class='code'>\n// Pass in the channels input1, input2, and input3, for the R,G, and B channels respectively.\nSurface mergeChannels (  Surface *input1,  Surface *input2,  Surface *input3  ){\n\tSurface resultSurface( input1->clone() );\n\tSurface::Iter iterResult = resultSurface.getIter( getWindowBounds() );\n\tSurface::Iter iter1 = input1->getIter( getWindowBounds() );\n\tSurface::Iter iter2 = input2->getIter( getWindowBounds() );\n\tSurface::Iter iter3 = input3->getIter( getWindowBounds() );\n\twhile( iterResult.line() && iter1.line() && iter2.line() && iter3.line() ) {\n\t\twhile( iterResult.pixel() && iter1.pixel() && iter2.pixel() && iter3.pixel() ) {\n\t\t\titerResult.r() = iter1.r();\n\t\t\titerResult.g() = iter2.g();\n\t\t\titerResult.b() = iter3.b();\t\t\n\t\t}\n\t}\n\treturn resultSurface;\t\t\n}\t\t\t\t\t</pre>\n\t\t\t\t\t\t\n\t\t\t</div>\n\t\t\t\n\t\t\t<div id='ccp_p1_results'>\n\t\t\t\t<h4>Results</h4>\n\t\t\t\t\t<p>The following diagram depicts which input images are mapped to which pixel values. For example, in Output 1, the red pixels are taken from input1, the green pixels are taken from input2, and the blue pixels are taken from input3.</p>\n\t\t\t\t\t<img src='/images/project_pics/ccp/p1_files/outputDiagram.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t\t\t\n\t\t\t\t\t<p>The results of channelMixing are shown below (in accordance to the above diagram). Please open the images in a new tab (right click or drag to tab bar in modern browsers) for to view full-size.</p>\n\t\t\t\t\t\n\t\t\t\t\t<p>Scene 1:</p>\n\t\t\t\t\t<img src='/images/project_pics/ccp/p1_files/scene1.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t\t\t<img src='/images/project_pics/ccp/p1_files/output1.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t\t\t\n\t\t\t\t\t<p>Scene 2:</p>\n\t\t\t\t\t<img src='/images/project_pics/ccp/p1_files/scene2.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t\t\t<img src='/images/project_pics/ccp/p1_files/output2.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t\t\t\n\t\t\t\t\t<p>Scene 3:</p>\n\t\t\t\t\t<img src='/images/project_pics/ccp/p1_files/scene3.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t\t\t<img src='/images/project_pics/ccp/p1_files/output3.jpg' alt='Scene' width='600px' align='center'/>\n\t\t\t\t\t\n\t\t\t\t</div>\n\t\t\t\t\t\n\t\t\t\t<div id='ccp_p1_source'>\n\t\t\t\t\t<h4 >Source</h4>\n\t\t\t\t\t<p>The source code and all image files can be <a href='/images/project_pics/ccp/p1_files/ccp_channelMixing.zip'>downloaded here</a>. Note - to make sure your cinder libraries are setup correctly, I recommend making a new Cinder project using <a href='http://libcinder.org/docs/welcome/MacNewProject.html#tinderbox'>TinderBox</a> and copying the source code into that.</p>\n\t\t\t\t\t<p>Have fun with your channelMixing adventures!</p>\n\t\t\t\t</div>", 
                    "order": 1, 
                    "parent": 6, 
                    "pic": "blank", 
                    "slug": "channelmixing", 
                    "summary": "Color mixed images from multispectral illumination.", 
                    "title": "Channel Mixing"
                }, 
                {
                    "html": "<div id='tableOfContents'>\n\t\t\t\t<h4>Contents</h4>\n\t\t\t\t<ol>\n\t\t\t\t\t<li><a href='#ccp_p2_intro'>lightField Photography Intro</a></li>\n\t\t\t\t\t<li><a href='#ccp_p2_setup'>Software Setup</a></li>\n\t\t\t\t\t<li><a href='#ccp_p2_photos'>Test Photos</a></li>\n\t\t\t\t\t<li><a href='#ccp_p2_mixing'>Data Capture</a></li>\n\t\t\t\t\t<li><a href='#ccp_p2_results'>Results</a></li>\n\t\t\t\t\t<li><a href='#ccp_p2_source'>Source</a></li>\n\t\t\t\t</ol>\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Intro -->\n\t\t\t<div id='ccp_p2_intro'>\n\t\t\t\t<h4>lightField Photography Intro</h4>\n\t\t\t\t<p>Project number 2 sets us on the task of digital refocusing -- the idea is the same as the one behind the recently popular <a href='http://www.lytro.com/'>Lytro</a> camera. Digital refocusing is a technique commonly seen in <a href='http://en.wikipedia.org/wiki/Light_field'>lightfield</a> photography, which is the technique of capturing all light passing through a given surface (the camera aperture) and adjusting image parameters (focus, exposure effects, etc) in post-production.</p>\n\t\t\t\t<p> While there are many technically impressive techniques that enable lightfield photography in a single camera (<a href='http://en.wikipedia.org/wiki/Plenoptic_camera'>microlens arrays</a>, etc) the same is possible using inexpensive camera technologies and manually shifting the position of the camera for each shot. To simplify the process, many researchers opt to make <a href='http://graphics.stanford.edu/papers/CameraArray/'>large camera arrays</a> - enabling quick capture of lightfield information that allows for digital refocusing, 3D effects, and more. The former (shifting the camera) is the technique explored here for capturing lightfield information.</p>\t\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Setup -->\n\t\t\t<div id='ccp_p2_setup'>\n\t\t\t\t<h4>Software Setup</h4>\n\t\t\t\t<p>Following from <a href='/portfolio/ccp/channelmixing'>Project 1</a>, I will again use the <a href='http://libcinder.org/'>Cinder Library</a> for my image processing needs. The general algorithm for digital refocusing is a simple one. We will take a series of pictures each horizontally offset by a fixed increment. Digital refocusing can be achieved by taking each picture, shifting the picture horizontally by some amount, and then averaging the pixel values of each image together. There are three basic functions the software performs:</p>\t\n\t\t\t\t<ol>\n\t\t\t\t\t<li>Import the photos</li>\n\t\t\t\t\t<li>Shift each photo</li>\n\t\t\t\t\t<li>Sum the photos</li>\n\t\t\t\t</ol>\n\t\t\t\t<p>The photos are imported in the main setup() function. The two functions of interest are thus titled shiftSurf and addSurfaces. shiftSurf takes a single image file (Surface) and slides the pixels to the left or right by a value designated by the variable shiftBy. Pixels are clamped to the edge of the surface to avoid index errors when trying to shift beyond the boundary of the pixel array.</p>\t\n\t\t\t\t<pre class='code'>\nvoid code_LF_shiftApp::shiftSurf(Surface *inputSurface, float shiftBy){\n  Surface tempSurface( inputSurface->clone() );\n    \n  Surface::Iter iterTemp = tempSurface.getIter( getWindowBounds() );\n  Surface::Iter iterInput = inputSurface->getIter( getWindowBounds() );\n    \n  while( iterTemp.line() && iterInput.line()) {\n      while( iterTemp.pixel() && iterInput.pixel()) {\n          iterInput.r() = iterTemp.rClamped(shiftBy,0);\n          iterInput.g() = iterTemp.gClamped(shiftBy,0);\n          iterInput.b() = iterTemp.bClamped(shiftBy,0);\n      }\n  }\n}\t\t\t\t</pre>\n\t\t\t\t<p>addSurfaces iterates through each of the shifted Surfaces and successively adds their corresponding pixel values to a final output Surface.</p>\n\t\t\t\t<pre class='code'>\nvoid code_LF_shiftApp::addSurfaces (Surface *outputSurface){\n    Surface::Iter iterOutput = outputSurface->getIter( getWindowBounds() );\n    while( iterOutput.line() ) {\n        while( iterOutput.pixel() ) {\n            iterOutput.r() = 0;\n            iterOutput.g() = 0;\n            iterOutput.b() = 0;\n        }\n    }        \n    for(int ii = 0; ii<16; ii++){\n        Surface currentSurf = mySurface[ii];\n        Surface::Iter iterOutput = outputSurface->getIter( getWindowBounds() );\n        Surface::Iter iterCurrent = currentSurf.getIter( getWindowBounds() );\n        while( iterOutput.line() && iterCurrent.line()) {\n            while( iterOutput.pixel() && iterCurrent.pixel()) {\n                iterOutput.r() += iterCurrent.r()/16.0f;\n                iterOutput.g() += iterCurrent.g()/16.0f;\n                iterOutput.b() += iterCurrent.b()/16.0f;\n            }\n        }\n    }\n}\t\t\t\t</pre>\n\t\t\t\t\t\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Test Photos -->\n\t\t\t<div id='ccp_p2_photos'>\n\t\t\t\t<h4>Test Photos</h4>\n\t\t\t\t<p><a href='http://www.eecis.udel.edu/~yu/'>Professor Jingyi Yu</a> from the University of Delaware, a leading researcher in the field of computational photography, provides a set of sample images with which this code can be tested. The picture set is available here (as of 10/06/11):  <a href='http://www.eecis.udel.edu/~yu/Teaching/CISC829_S10/handouts/toyLF.zip'> http://www.eecis.udel.edu/~yu/Teaching/CISC829_S10/handouts/toyLF.zip</a>. 256 images are provided, but for the sake of simplicity, only the top row of 16 images is used in this analysis. The first and last images in the set are shown below.</p>\t\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p2_files/testImage_sample.jpg'/>\n\t\t\t\t<p>The above code yields the ability to digitally refocus this set of images. The resulting output, which confirms the algorithm used, is given below. As one can see, we can digitally focus on the background, midground, and foreground.</p>\n\t\t\t\t<img width = '600px' src='/images/project_pics/ccp/p2_files/testResults.jpg'/>\n\t\t\t\t\n\t\t\t\t\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Data Capture -->\n\t\t\t<div id='ccp_p2_mixing'>\n\t\t\t\t<h4>Data Capture</h4>\n\t\t\t\t<p>To capture my own lightfield photos, I've setup a Nikon D7000 along a marked surface. The camera is shifted one inch to the right for each photo and a total of 16 photos are taken. The first and last picture of the set are shown below:</p>\t\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p2_files/myImage_sample.jpg'/>\t\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Results -->\n\t\t\t<div id='ccp_p2_results'>\n\t\t\t\t<h4>Results</h4>\n\t\t\t\t<p>The results from applying the above algorithm to my captured images are given below. As you can see, the large inter-image spacing results in a motion blur type effect on the out-of-focus parts. Furthermore, it gives us a very short effective depth of field, as can be seen from the fact that we can selectively focus on many foreground objects.</p>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p2_files/lightFieldResults.jpg'/>\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Source -->\n\t\t\t<div id='ccp_p2_source'>\n\t\t\t\t<h4>Source</h4>\n\t\t\t\t<p>The source code and all image files can be <a href='/images/project_pics/ccp/p2_files/ccp_lightFieldPhotography.zip'>downloaded here</a>.</p>\n\t\t\t\t\t<p>Have fun with your lightField adventures!</p>\n\t\t\t</div>", 
                    "order": 2, 
                    "parent": 6, 
                    "pic": "blank", 
                    "slug": "lightfield", 
                    "summary": "Lightfield photography with a standard digital camera.", 
                    "title": "Lightfield Photography"
                }, 
                {
                    "html": "<div id='tableOfContents'>\n\t\t\t\t<h4>Contents</h4>\n\t\t\t\t<ol>\n\t\t\t\t\t<li><a href='#ccp_p4_intro'>Multi-Chromatic Flash Intro</a></li>\n\t\t\t\t\t<li><a href='#ccp_p4_setup'>Software Setup</a></li>\n\t\t\t\t\t<li><a href='#ccp_p4_results'>Multi-Chromatic Flash Results</a></li>\n\t\t\t\t\t<li><a href='#ccp_p4_source'>Source</a></li>\n\t\t\t\t</ol>\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Intro -->\n\t\t\t<div id='ccp_p4_intro'>\n\t\t\t\t<h4>Multi-Chromatic Flash Intro</h4>\n\t\t\t\t<p>Project 4 explores the realm of computational illumination. Rather than tweaking our capture parameters, we can tweak our environment parameters (in this case the camera flash) to create similarly fascinating results. The underlying principal is that we now have a camera flash that is not a single burst of white light, but discrete bursts of red, green, and then blue light in sequential order. Given that the exposure time of the shot is longer than the full sequence of red, green, and blue flashes, regular still imagery will appear no different. The three light colors will be added together such that the final result appears the same as a white-lit scene. Moving objects, however, will appear quite different. Because they will be at a different position in space at each flash color, a color trail of it's motion will appear. So the end result is thus: still objects will appear no different than if they had be shot in white light, while moving objects will have a distinct color trail detailing their motion. Importantly, the exact contour and size of the color trail can be used in interesting ways to determine properties of the motion of the object.</p>\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Software Setup -->\n\t\t\t<div id='ccp_p4_setup'>\n\t\t\t\t<h4>Software Setup</h4>\n\t\t\t\t<p>Unlike previous projects, where the software portion of the exercise focuses on editing the collected images, the software section here instead focusing on creating the right environment for the image collection. Namely, the software creates the temporally dynamic color flash. For a simple first approach, I used my 27' computer monitor as the flash source. This choice gave me a large, bright illumination source that could be easily controlled. Of course, the end goal is to have a stand-alone flash component that would actually connect direction to the camera, but this method will provide a good proof of concept.</p>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p4_files/monitorFlash.jpg'/>\n\t\t\t\t<p><a href='http://www.processing.org'>Processing</a> was used to control the screen's output. A large rectangle with the same dimensions as the monitor is created and the fill color of this rectangle is then changed at some time interval. The parameter totalFlashTime allows the user to set the flash time. Each color will be flashed for one-third of this value. The result is a bit jarring to look at, but it gets the job done. The simple code that runs the flash algorithm is given below:</p>\n\t\t\t\t<pre class='code'>\nint totalFlashTime = 100; //In milliseconds\nint stage = 1;\nint x = 2560; //horizontal pixels in the screen\nint y = 1440; //vertical pixels in the screen\n\nvoid setup(){\n  \nsize(x, y, P2D); //size of the processing window\n  \n}\n\nvoid draw(){\n  if(stage == 0){\n    fill(255, 0, 0); //flash red\n    rect(0, 0, x, y);\n  }else if(stage==1){\n    fill(0, 255, 0); //flash blue\n    rect(0, 0, x,y);\n  }else{\n    fill(0, 0, 255); //flash green\n    rect(0, 0, x, y);\n  }\n  stage = (stage+1)%3;\n  delay(totalFlashTime/3); \n}</pre>\t\n\t\t\t</div>\n\t\t\n\t\t\t<!-- Results -->\n\t\t\t<div id='ccp_p4_results'>\n\t\t\t\t<h4>Multi-Chromatic Flash Results</h4>\n\t\t\t\t<p>The resulting images have the distinct motion-color effect that we hoped to see.</p>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p4_files/multiChromFlash1.JPG'/>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p4_files/multiChromFlash2.JPG'/>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p4_files/multiChromFlash3.JPG'/>\n\t\t\t\t\n\t\t\t\t<p>We can see that the falling ball and crumpled pieces of paper leave a distinct color trail as opposed to just appearing blurry. In contrast, the surrounding background appears to have had standard white illumination. Small amounts of motion are easily detected, as can be seen in the first image, where the pieces of paper that have already fallen and hit the table can still be seen to have a color outline as they wiggle to a stop.</p>\n\t\t\t\t\n\t\t\t\t<p>We can also capture the motion of things that are moving in an oscillatory fashion (as opposed to just falling in one direction) - in this case, my hand and arm.  By analyzing the multiple red (or green or blue) color lines that are seen (especially in the second image), it is possible to determine the rate at which my arm was waving back and forth. </p>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p4_files/multiChromFlash4.JPG'/>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p4_files/multiChromFlash5.JPG'/>\n\t\t\t\t\n\t\t\t \t<p>Lastly, if we move the camera itself in the middle of an exposure, we can see that all objects will get a color-trailed blur.</p>\n\t\t\t \t<img width='600px' src='/images/project_pics/ccp/p4_files/multiChromFlash6.JPG'/>\n\t\t\t \t\n\t\t\t \t<p>While these images serve as a proof-of-concept for the idea, the more interesting part comes when we start using the information constrained in these trails to calculate properties of the objects in motion (speed, acceleration, frequency, etc). Explorations in this direction will be coming in the following weeks - stay tuned.</p>\n\t\t\t</div>\t\n\t\t\t\n\t\t\t\n\n\n\t\t\t<!-- Source -->\n\t\t\t<div id='ccp_p4_source'>\n\t\t\t\t<h4>Source</h4>\n\t\t\t\t<p>The source code and all image files can be <a href='/images/project_pics/ccp/p4_files/ccp_multiChromFlash.zip'>downloaded here</a>.</p>\n\t\t\t\t<p>Have fun!</p>\n\t\t\t</div>", 
                    "order": 3, 
                    "parent": 6, 
                    "pic": "blank", 
                    "slug": "multichromatic", 
                    "summary": "Motion tracking with multichromatic computational illumination.", 
                    "title": "Multichromatic Flash"
                }, 
                {
                    "html": "<div id='tableOfContents'>\n\t\t\t\t<h4>Contents</h4>\n\t\t\t\t<ol>\n\t\t\t\t\t<li><a href='#ccp_p3_intro'>multiFlash Imaging Intro</a></li>\n\t\t\t\t\t<li><a href='#ccp_p3_setup'>Software Setup</a></li>\n\t\t\t\t\t<li><a href='#ccp_p3_multiflash'>Multi-flash Technique</a></li>\n\t\t\t\t\t<li><a href='#ccp_p3_multicolor'>Multi-color Technique</a></li>\n\t\t\t\t\t<li><a href='#ccp_p3_source'>Source</a></li>\n\t\t\t\t</ol>\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Intro -->\n\t\t\t<div id='ccp_p3_intro'>\n\t\t\t\t<h4>multiFlash Imaging Intro</h4>\n\t\t\t\t<p>Project 3 takes a look at a solution to the computer vision challenge of determining 3D objects from highly textured, complex images. The solution we'll be looking at to solve this is multi-flash imaging. A more in-depth introduction to the technique, as well as many references, can be found at Ramesh Raskar's site for <a href='http://web.media.mit.edu/~raskar/NprCamera/'>non-photorealistic imaging</a>.</p>\n\t\t\t\t\n\t\t\t\t<p> The simple principle behind this technique is to take multiple images of a scene, with each image being illuminated from a different location. This will create the effect of having multiple camera flashes located at different spots about the camera. This produces multiple images each with their own unique shadowing. The variances in this shadowing can then be used to determine 3D objects from the textured background of a scene. A quick example (as taken from Raskar's above site) can be seen by applying this technique to photographs of an engine. </p>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p3_files/engineExample.jpeg'/>\n\t\t\t\t<p>As the last two panels show, the multi-flash technique (3rd panel) creates a clear, understandable 3D 'sketch' of the engine, while a simple edge-detection algorithm (4th panel) produces a mess of contours and textures.</p>\n\t\t\t\t<p> Furthermore, the same effect can be produced by illuminating a scene with three differently colored sources, each in a different location. By taking a single image and isolating the R,G, and B channels, three distinct shadows can be obtained and the same algorithm as used for the multi-flash technique can be applied. In this project, I explore both routes: multi-flash and multi-color.</p>\n\t\t\t</div>\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t<!-- Software Setup -->\n\t\t\t<div id='ccp_p3_setup'>\n\t\t\t\t<h4>Software Setup</h4>\n\t\t\t\t<p>Generously, much of the source code used for this project has been provided <a href='http://vision.ai.uiuc.edu/~tankh/NPRCameraSrc.tar.gz'>here</a> by <a href='http://vision.ai.uiuc.edu/~tankh/'>Kar-Han Tan</a> (also note that all of my source code for this project can be <a href='#ccp_p3_source'>downloaded below</a>). My code is Matlab based and split into three  files:\n\t\t\t\t<ul>\n\t\t\t\t\t<li><pre>FindDepthEdges_flashes.m  //main application for the multi-flash technique</pre></li>\n\t\t\t\t\t<li><pre>FindDepthEdges_color.m  //main application for the multi-color technique</pre></li>\n\t\t\t\t\t<li><pre>hysteresis_thresholding.m  //implements thresholding algorithm</pre></li>\n\t\t\t\t</ul>\t\t\t\n\t\t\t\t</p>\n\t\t\t\t\n\t\t\t\t<p>The two FindDepthEdges files setup the environment by importing the image files, rendering the appropriate grayscale bitmaps, and calling the edge-detection functions. The main difference between the two files is how they handle the initial image import. FindDepthEdges_flashes.m imports four separate images, \n\t\t\t\t<pre class='code'>\nimg1 = imread('up.jpg');\nimg2 = imread( 'right.jpg' );\nimg3 = imread( 'down.jpg' );\nimg4 = imread( 'left.jpg' );</pre>\n\t\t\t\twhile FindDepthEdges_color.m imports a single image file and then splits the channels of that file into three distinct bitmap objects representing the red, green, and blue channels.\n\t\t\t\t<pre class='code'>\ncolorImage = imread('colorIm3.jpg'); \nimgR = colorImage(:,:,1);\nimgG = colorImage(:,:,2);\nimgB = colorImage(:,:,3);</pre>\n\t\t\t\t</p>\n\t\t\t\t\n\t\t\t\t<p> Note that in the multi-flash technique, we import four images (up, down, left, right), while we only import three for the multi-color technique (red, green, blue). The code in each FindDepthEdges file is changed accordingly to incorporate these differences. </p>\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Multi-Flash -->\n\t\t\t<div id='ccp_p3_multiflash'>\n\t\t\t\t<h4>Multi-flash Technique</h4>\n\t\t\t\t<p>For the multi-flash technique I used a setup fabricated by <a href='http://mit2011.luescher.org/blog/'>Sam Luescher</a> that creates an LED flash in four different locations on a one-second interval. The results of which are given below (excuse the anger - proof I usually smile: <a href='p3_files/scienceSmile.jpg'>Science!</a>)</p>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p3_files/face_input.jpg'/>\n\t\t\t\t<p>Applying the multi-flash technique to these input photos provides the following results:</p>\n\t\t\t\t<img width='600px' src='/images/project_pics/ccp/p3_files/face_results.jpg'/>\n\t\t\t\t<p>The confidence image (left panel) shows clearly the 'sketch' effect that is seen in other applications of this algorithm, while the outline image (right panel) produces a pretty good outline of my face and ignores the background texture. In the left panel, also note the detection of fine hairs at the top of my head, showing the accuracy of such a technique to determine fine qualities. The teeth show some undesired effects though as the LED flash causes a different reflection from the teeth in each image, resulting in a blurry artifact.</p>\n\t\t\t</div>\n\t\t\t\n\t\t\t<!-- Multi-Color -->\n\t\t\t<div id='ccp_p3_multicolor'>\n\t\t\t\t<h4>Multi-color Technique</h4>\n\t\t\t\t<p>For the multi-color technique, <a href='http://aydinarpa.com/'>Aydin Arpa</a> and I setup three desk lamps each with a different filter creating distinct red, green, and blue light sources. We captured images of both my hand and a conference phone that had been in our 'studio room'. Unfortunately I've found the multi-color technique to provide less desirable results that the multi-flash technique. Using only three images diminishes the quality of the results (recall we had four in the multi-flash technique) simply because we have fewer data points to help the algorithm locate edges, and moreover, our home-made RGB light studio was likely not perfectly calibrated to illuminate all contours sufficiently. Nonetheless, the results are still intriguing. The results for the multi-color illuminated hand are below:</p>\n\t\t\t\t<img width='600' src='/images/project_pics/ccp/p3_files/fakeHand.jpg'/>\n\t\t\t\t<p>The image on the left is the original input source image of my hand. The middle image is the confidence map and the third, rightmost image is the edge map that is generated. The scanned image of a hand is placed next to my real hand to show that the algorithm will not pick up edges from flat textures. The algorithm works quite well in this regard, as the outline of my hand is clearly seen in the third panel, while the scanned image of a hand produces no outline. Unfortunately, the three sources did not provide enough contour data to create a closed outline of my hand. To fix this, either the multi-flash technique needs to be used, or the RGB light sources need to be repositioned to more appropriately capture the shadow information. </p>\n\t\t\t</div>\n\t\t\t\n\t\t\t<p>We also captured an RGB illuminated image of a conference phone. We chose this object because of it's many complex ridges, textures, and buttons. The results came out nicely as seen below.</p>\n\t\t\t<img width='600px' src='/images/project_pics/ccp/p3_files/polycom.jpg'/>\n\t\t\t<p>Again, the leftmost image is the original image, the middle panel is the confidence map, and the third panel is the edge map that is generated. Also notice that, similar to the hand-image, the final edge map in the third panel does not form a closed contour map - some of the edges of the phone are missing as the shadows correlating to these edges were not completely captured. The ridges of the middle speaker are very nicely captured though, showing again the power of this algorithm to capture fine details.</p>\n\t\t\t\n\t\t\t<!-- Source -->\n\t\t\t<div id='ccp_p3_source'>\n\t\t\t\t<h4>Source</h4>\n\t\t\t\t<p>The source code and all image files can be <a href='/images/project_pics/ccp/p3_files/ccp_multiFlashImaging.zip'>downloaded here</a>.</p>\n\t\t\t\t<p>Have fun with your multiFlash explorations!</p>\n\t\t\t</div>", 
                    "order": 4, 
                    "parent": 6, 
                    "pic": "blank", 
                    "slug": "multiflash", 
                    "summary": "Image reconstruction through multiflash imaging techniques.", 
                    "title": "Multiflash Imaging"
                }, 
                {
                    "html": "<div id='tableOfContents'>\n\t<h4>Contents</h4>\n\t<ol>\n\t\t<li><a href='#ccp_p5_intro'>Multi-Chromatic Motion Intro</a></li>\n\t\t<li><a href='#ccp_p5_method'>Method</a></li>\n\t\t<li><a href='#ccp_p5_Implementation'>Implementation</a></li>\n\t\t<li><a href='#ccp_p5_results'>Results</a></li>\n\t</ol>\n</div>\n\t\t\t\n<!-- Intro -->\n<div id='ccp_p5_intro'>\n\t<h4>Multi-Chromatic Motion Intro</h4>\n\t<p>There are many methods for estimating the motion of objects in a captured photographic scene. Understanding the properties of moving objects in a photograph is useful for understanding a scene more thoroughly and can be used for most complex computational photography techniques, such as deblurring. The most common approach to solving this problem requires the analysis of several sequential frames from a given scene. Thus, in order to avoid losing significant information regarding the motion of the object of interest, many short-exposure captures are used.</p>\n\n\t<p>The technique of capturing many short-exposure sequential frames to com- pute optical flow introduces two main problems: 1) this technique performs poorly in low-light scenarios where longer exposures are desired, and 2) motion paths that are redundant (i.e. trace over the same location or locations) intro- duce ambiguity into the calculation of direction vectors of the moving object.</p>\n\n\t<p>One method that offers to solve such problems is to use a multichromatic illumination with single-exposure capture. That is, rather than illuminating the scene with steady white light, a multichromatic illumination source is used. This multichromatic source changes in color temporally such that objects in motion are illuminated with a different color at each position in their path. Furthermore, the colors used to illuminate the scene can be chosen such that stationary objects are illuminated equally over the red, green, and blue channels of the camera sensor, causing still objects to appear as though illuminated by white light. An example image displaying this effect can be seen below, where a falling ball produces a color streak while stationary items appear to be illuminated by white light. By controlling the dynamics of the multichromatic flash and analyzing the produced color profile, optical flow properties can be calculated.</p>\n</div>\n\n<!-- Method -->\n<div id='ccp_p5_method'>\n\t<h4>Method</h4>\n\t<p>To explore this domain, a multichromatic flash system is created. The main components of this system consist of a camera, an RGB flash, and a linear actuator. Each of these components is remotely controllable.</p>\n\n\t<p>Several constrains are important to consider when choosing the camera sys- tem. Most importantly, the camera must precisely synchronize with the RGBlash. Thus, being able to accurately control the exposure time and shutter state is of high importance. Furthermore, given the controllability of the camera, the RGB flash must be similarly controllable such that the two components can be driven from a single processing unit. Thus, the state of each colored light source must be temporally controllable. An interfacing microcontroller device can then manage both components and precisely synchronize operation.</p>\n\n\t<img width='600px' src='/images/project_pics/ccp/p5_files/motion_img1.jpg'/>\n\t<p></p>\n\n\t<p>In order to obtain quantitative results, a remotely controllable linear ac- tuator is also needed. The linear actuator will serve as the moving object of interest in our captured scene. By having fine-grain control over the motion of the linear actuator, it is possible to compare the computed motion properties to the known real-world dynamics. Again, the remote controllability of the linear actuator allows us to synchronously drive the actuator to match the timing of the camera and multichromatic flash.</p>\n\n\t<p>The step-by-step routine that the microcontroller will run in order to capture the images is as follows:\n\t\t<ol>\n\t\t\t<li>Trigger camera auto-focus</li>\n\t\t\t<li>Open camera shutter</li>\n\t\t\t<li>Begin R,G,B illumination sequence</li>\n\t\t\t<li>Begin linear actuator motion</li>\n\t\t\t<li>Close camera shutter</li>\n\t\t\t<li>Reset linear actuator</li>\n\t\t</ol>\n\t</p>\n\n</div>\n\n<!-- Implementation -->\n<div id='ccp_p5_Implementation'>\n\t<h4>Implementation</h4>\n\t<u>Microcontroller</u>\n\t\t<p>An Arduino Uno microcontroller is used to control the operation of the discrete components of this test system. The Arduino Uno allows precise control over triggering the camera shutter, multichromatic flash, and linear actuator, enabling synchronous operation. The UNO can be power by a 9V battery, allowing the multichromatic flash system to operate independently of any wall- sourced power.\n\t\t</p>\n\n\t<u>Camera</u>\n\t\t<p>The camera used in this test setup is a Canon SD750. While this model does not have built-in support for external flashes, an open source library, called CHDK (Canon Hack Development Kit), enables the camera to be remotely controlled by external circuitry. Thus, using the Arduino Uno, we can simultaneously trigger the camera shutter and multichromatic flash independently, in effect granting external-flash capability.</p>\n\t\t\n\t\t<p>In order to interface the Camera to the Arduino board, a USB cable is used. One side of the USB cable is cut and the individual pins are tied to digital IO pins on the Arduino board. The other side of the cable is connected to the standard camera USB port.</p>\n\n\t<u>Multichromatic Flash</u>\n\t\t<p>Three Luxeon Rebel LEDs are used to create the multichromatic (in this case, tri-color) flash. Warm-white LEDs are used and red, green, and blue filter sheets are placed over each LED. The LEDs are driven by a simple switching circuit that interfaces to the Arduino board. Each LED can be turned on and off independently of the other LEDs.</p>\n\t\t<img width='600px' src='/images/project_pics/ccp/p5_files/motion_img2.jpg'/>\n\t\t<p></p>\n\n\t<u>Linear Actuator</u>\n\t\t<p>The linear actuator is used to create a repeatable motion for a controlled setup over many captures. A motorized potentiometer is used. A bracket is driven by a DC motor, which is controlled by an Arduino board, and the position of the bracket is given as a potentiometer reading. Thus, a feedback loop can be designed to drive the bracket to any position. One challenge associated with this technique is defining a known speed for the moving bracket. While the DC motor can only control the speed of the bracket through a PWM value. There is no obvious correlation to any real unit scale. To overcome this challenge, a series of PWM settings were chosen and tested for their associated speeds. A software timer is used to measure the time taken to travel the entire length of the linear actuator. The measured time is averaged over many runs for a given setting and the corresponding speed can be determined. Furthermore, using this data, a photo can be produced that captures the full range of motion by choosing a travel time that matches the exposure time of the camera.</p>\n\n\t\t<img width='600px' src='/images/project_pics/ccp/p5_files/motion_img3.jpg'/>\n\t\t<p></p>\n\t\t<img width='600px' src='/images/project_pics/ccp/p5_files/motion_img4.jpg'/>\n\t\t<p></p>\n\n\t<u>Enclosure</u>\n\t\t<p>An enclosure is fabricated to hold the components involved in the system, ex- cluding the linear actuator. The Arduino board, LED driving circuitry, and RGB LEDs are contained by the enclosure. A simple snap-fit box design is used.</p>\n\n\t\t<img width='600px' src='/images/project_pics/ccp/p5_files/motion_img5.jpg'/>\n\t\t<p></p>\n\n\t\t<p>The box conveniently hides away many of the more cumbersome components, and has a clean two-cable interface on it\u2019s rear consisting of the camera USB cable and a cable to connect to the linear actuator.</p>\n\n\n\t<u>Full System</u>\n\t\t<img width='600px' src='/images/project_pics/ccp/p5_files/motion_img6.jpg'/>\n\t\t<p></p>\n\n\n\n\n\n</div>\t\n\n<!-- Results -->\n<div id='ccp_p5_results'>\n\t<h4>Results</h4>\n\t\t<p>The final system uses an exposure time of 1.5 seconds and the images are cap- tured in a low-light setting. Comparing these two images, the ad- ditional information carried in the multichromatic flash image can be clearly appreciated.</p>\n\n\t<img width='600px' src='/images/project_pics/ccp/p5_files/motion_img7.jpg'/>\n\t<p></p>\n\n\t<u>Image Processing</u>\n\t<p>Matlab is used to perform the image processing tasks required to compute the motion properties of the object in this capture. The code used to perform this analysis can be found in Section 7. To prepare the images for calculation, a simple list of operations is performed:\n\t\t<ol>\n\t\t\t<li>Channel isolation</li>\n\t\t\t<li>Black/White Thresholding </li>\n\t\t\t<li>Hole Filling</li>\n\t\t\t<li>Blob Isolation</li>\n\t\t</ol>\n\t</p>\n\n\t<p>After these steps are performed, we have three distinct blobs that represent the blurred position of the moving object at the three time intervals. The ex- trema and centroid of these blobs can then be calculated in order to analyze the positional differences that will allow the calculation of optical flow properties. Each of these steps, as well as the extrema and centroid calculation are shown below. Given that we know certain properties of the system, such as the order in which the illumination sources are triggered and the duration for which they remain on, we can begin to analyze the motion of the object.</p>\n\n\t<img width='600px' src='/images/project_pics/ccp/p5_files/motion_img8.jpg'/>\n\t<p></p>\n\n\t<u>Direction</u>\n\t<p>The most obvious claim that can be made from this data is the direction of motion. By taking the centroid positions of each blob, we can calculate and draw a direction vector that connects the start and end positions. A vector from the red-centroid to the blue-centroid can be drawn, or more fine grain detail can be gathered by using the intermediary green-centroid to produce two direction vectors: one for the first interval of travel and one for the second. See image below.</p>\n\t\n\n\n\t<u>Length</u>\n\t<p>Given the position of each blob centroid, we are able to determine the lengths of each vector connecting the three regions. These lengths are the first step in determining properties such as speed and acceleration. The lengths of the given vectors, in pixels, for the sample image being used are shown below.</p>\n\n\t<u>Speed</u>\n\t<p>Using the known timing of the individual LED flashes and the length of each motion vector. We can now trivially calculate the velocity of the blob at each stage of its motion. Note that because we are calculating from the center of each blob, we effectively lose the first sixth and last sixth of total motion in this calculation. The speed values calculated for the sample image is shown below.</p>\n\n\t<img width='400px' src='/images/project_pics/ccp/p5_files/motion_img9.jpg' style='padding-left: 100px'/>\n\t<p></p>\n\n\t<u>Acceleration</u>\n\t<p>Finally, as can be inferred from the differing vector lengths, the linear actuator does not have a uniform velocity throughout its path. Thus, there is some accel- eration term that can be similarly calculated. Knowledge of the flash timing is again leveraged and an average acceleration term for the period of time between the red and blue LED firing can be calculated. </p>\n\n</div>", 
                    "order": 5, 
                    "parent": 6, 
                    "pic": "blank", 
                    "slug": "multichromatic_motion", 
                    "summary": "Motion tracking with multichromatic computational illumination.", 
                    "title": "Multichromatic Motion"
                }
            ]
        }, 
        {
            "html": "<p>Point & Shoot Data explores the use of visible light as a wireless communication medium for mobile devices. A snap-on case allows users to send messages to other mobile devices based on directionality and proximity. No email address, phone number, or account login is needed, just point and shoot your messages!</p>\n\n<img src='/images/project_pics/psd1.jpeg' width='600px' align='center'/>\n&nbsp;\n\n<p>The realm of wireless communications has primarily, by design, been confined to invisible radio waves. This limits a user's ability to understand what data signals are, how they carry information, how their performance can be modified, or how broken communication links can be debugged. In certain applications, a visible communications medium may provide a more intuitive and meaningful user experience. To explore this concept, we use visible light to transmit information due to its deeply rooted role in human perception and non-verbal communication. </p>\n\n<h3>Hardware</h3>\n</br>\n<p>The communication technology consists of the transmitting and receiving hardware, the device-hardware interface, and the software-implemented communication protocol. The underlying principle of visible light communication is that an LED is modulated at high rates (above human perception). That is, the intensity of the light emitted from the LED changes as a function of time. A light sensitive photodiode device is then used to sense these changes in light intensity. For a simple digital communication system, an LED in the 'on' state can represent a binary 1, while an LED in the 'off' state can represent a binary 0.</p>\n\n<p>The hardware implementing this protocol (LED, photodiode, analog circuitry, microprocessor) can be seen below:</p>\n<img src='/images/project_pics/psd2.jpg' width='600px' align='center'/>\n&nbsp;\n\n<p>The case designed to hold the hardware and the phone is shown below.</p>\n<img src='/images/project_pics/psd3.png' width='600px' align='center'/>\n&nbsp;\n\n<h3>Interface</h3>\n</br>\n<p>The physical interaction with the communications medium is one of vital interest to this project. Thus, the interaction with the software abstraction of the data carried on that medium is also highly important. We design the user interface to match the intuitive human interactions that are enabled by using visible light. Our prototype is a simple note-passing application that employs a sliding motion to 'push' the data from one device to another, in much the same way that a person can physically slide or push a note to a nearby friend. This motion takes advantage of both the directionality and physicality of our light-based communications medium.</p>\n\n<img src='/images/project_pics/psd4.png' width='600px' align='center'/>\n&nbsp;\n\n<p>A quick testing video of the communication hardware and interface can be seen below. Note that the phones are not in their cases for this test, but the LED and photodiode hardware can be seen above each of the phones (watch for the LED to dim on message transfer).</p>\n<iframe src='http://player.vimeo.com/video/37823884' width='600' height='337' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n&nbsp;\n\n<h3>Affordance Tools</h3>\n</br>\n<p>We also create and explore a series of affordance tools to enable the user to directly manipulate the communication medium. We take advantage of lenses, filters, and mirrors in our design. The tools demonstrate the ability of a user to directly manipulate the communication channel. Further design work on optimizing usability is needed.</p>\n<img src='/images/project_pics/psd5.png' width='600px' align='center'/>\n&nbsp;\n\n<p>The enclosure features an interface that allows physical affordance tools to be placed in front of the LED and photodiode.</p>\n<img src='/images/project_pics/psd6.png' width='600px' align='center'/>\n&nbsp;\n\n<p>We also explore a color filter that allows users to create separate communication channels. The filters can be placed in front of both the transmitter and receiver such that a device can, for instance, select to listen to only a blue or red channel.  White light may then be considered a 'public' channel.</p>\n<img src='/images/project_pics/psd7.png' width='600px' align='center'/>\n&nbsp;\n\n<p>Furthermore, it should be noted that the user's body is a very important mechanism for controlling the communication medium. The hand can be used to cover the LED (blocking all transmission) or cover the photodiode (blocking all incoming data). This natural behavior maps intuitively to how people interact with physical information written on paper or a computer screen; private data can easily be covered by a hand or turned away from peering parties. </p>\n\n<h3>Construction and Testing Pictures</h3>\n</br>\n<img src='/images/project_pics/psd8.JPG' width='600px' align='center'/>\n&nbsp;\n<img src='/images/project_pics/psd9.jpeg' width='600px' align='center'/>\n&nbsp;\n<img src='/images/project_pics/psd10.jpeg' width='600px' align='center'/>\n&nbsp;\n<img src='/images/project_pics/psd11.jpeg' width='600px' align='center'/>\n\n\n\n\n\n<p>Point & Shoot Data was done in collaboration with <a href='http://slcl.ca'>Stephanie Lin</a>, <a href='http://shaunsalzberg.com/'>Shaun Salzberg</a>, and <a href='http://mit2011.luescher.org/about/'>Sam Luescher</a> at the <a href='http://media.mit.edu'>MIT Media Lab</a>.</p>\t", 
            "largePic": "nav/psd_lrg.jpg", 
            "order": 7, 
            "slug": "psd", 
            "smallPic": "nav/psd.jpg", 
            "summary": "Mobile-to-mobile directional communication with light.", 
            "title": "point&Shoot", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>dotPlot provides a kit of parts that enables people to create dynamic 3D drawing utensils.</p>\n\t\t\t\n\t\t\t<iframe src='http://player.vimeo.com/video/30696793?title=0&amp;byline=0&amp;portrait=0' width='600' height='338' frameborder='0' webkitAllowFullScreen allowFullScreen></iframe>\n\t\t\t\n\t\t\t<p><strong>Create</strong> ... tools with natural affordance ... dynamic 2D drawings with 3D tools ... something fun</p>\n\t\t\t<p><strong>Share</strong> ... your kit of parts to collaboratively create tools ... your digital drawings ... your drawing space</p>\n\t\t\t<p><strong>Evolve</strong> ... your tool as you draw ... your tool into something more complex (or more simple)</p>\n\t\t\t<p></p>\t\n\t\t\t<img src='/images/project_pics/dotplot_1.jpeg' width='600px' align='center'/>\n\t\t\t<p></p>\t\t\t\n\t\t\t<img src='/images/project_pics/dotplot_2.png' width='600px' align='center'/>\n\t\t\t<p></p>\t\n\t\t\t<p>dotPlot was done in collaboration with <a href='http://www.colorsaregood.com/'>Valentine Heun</a>, <a href='http://slcl.ca'>Stephanie Lin</a>, and <a href='http://mit2011.luescher.org/about/'>Sam Luescher</a> at the <a href='http://media.mit.edu'>MIT Media Lab</a>.</p>", 
            "largePic": "nav/dotplot_lrg.jpeg", 
            "order": 3, 
            "slug": "dotplot", 
            "smallPic": "nav/dotplot.jpeg", 
            "summary": "A kit of parts to create dynamic 3D drawing utensils", 
            "title": "dotPlot", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>influorescence (inflorescence + fluorescence) is a two-way tangible interface for ambient lighting control and power monitoring.</p>\n\t\t\t\n\t\t\t<iframe src='http://player.vimeo.com/video/32718508?title=0&amp;byline=0&amp;portrait=0' width='600' height='338' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n\t\t\t<!-- <p> <strong>Problem:</strong> Shared resources don't gain the sympathy of the community that uses it (e.g. earth's environment, shared living spaces, etc). The responsibility falls too lightly across the shoulders of all users, and no one person feels sufficiently responsible to maintain the resource.</p>\n\t\t\t<p><strong>Solution:</strong> We distill the effects of consumption to a personal level. In this case, your energy consumption directly influences your own personal 'ecosystem'.</p>\n\t\t\t -->\n\t\t\t \n\t\t\t<p><strong>The Project:</strong> </p>\n\t\t\t<p>- We take the environment as the shared resource of interest.</br>\n\t\t\t- Individuals' daily energy consumption creates unseen influences on the environment.</br>\n\t\t\t- This unseen influence is too abstract to produce an effective call to action.</br>\n\t\t\t- Our goal is to create an organic form that displays your relationship to your energy use.</br>\n\t\t\t- We explore the relationship between life (the organic form) and consumption.</p>\n\t\t\t\t\n\t\t\t\n\t\t\t<p><strong>The Interface:</strong> </p>\n\t\t\t<p>A garden will represent the organic form and the brightness of an LED light system will be the energy consumer. Higher energy consumption in the lighting system will lead to a tangible garden display that appears less healthy.</p>\n\t\t\t<p>The lights can be directly controlled by a light switch and the garden will react accordingly. Or, the garden can be directly manipulated (groomed or killed) to in turn influence the state of the lighting system.</p>\n\t\t\t<img src='/images/project_pics/infloPic1.png' width='600px' align='center'/>\n\t\t\t<p></p>\n\t\t\t<img src='/images/project_pics/infloPic2.png' width='600px' align='center'/>\n\t\t\t<p></p>\n\t\t\t<img src='/images/project_pics/infloPic3.jpeg' width='600px' align='center'/>\n\t\t\t<p></p>\n\t\t\t<img src='/images/project_pics/infloPic4.jpeg' width='600px' align='center'/>\n\t\t\t\n\t\t\t\n\t\t\t<p>influorescence was done in collaboration with <a href='http://www.colorsaregood.com/'>Valentine Heun</a>, <a href='http://slcl.ca'>Stephanie Lin</a>, and <a href='http://shaunsalzberg.com/'>Shaun Salzberg</a> at the <a href='http://media.mit.edu'>MIT Media Lab</a>.</p>", 
            "largePic": "nav/inflo_lrg.jpeg", 
            "order": 5, 
            "slug": "influorescence", 
            "smallPic": "nav/inflo.jpeg", 
            "summary": "A two-way tangible interface for ambient lighting control and power monitoring", 
            "title": "influorescence", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>The Boston University Smart Lighting Center, led by Professor Thomas Little, is an effort to leverage the unique properties of solid state lighting to create intelligent lighting systems. The project strives to embed communications technology into general purpose lighting - providing data connectivity wherever there is man-made light.</p>\n\t\t\t\n\t\t\t<img  src='/images/project_pics/smartlighting_1.jpeg' alt='' width='400px' style='padding-left: 130px;'/>\n\t\t\t\n\t\t\t <p>By merging the energy efficiency of LEDs, the ubiquity of indoor lighting, and the capability of freespace-optical communication we have the opportunity to offer a transformative paradigm capable of providing energy-efficient lighting and ubiquitous data connectivity.</p>\n\t\t\t\n\t\t\t<img  src='/images/project_pics/smartlighting_2.jpg' alt='' style='padding-left: 120px;'/>\n\t\t\t<img src='/images/project_pics/smarttravel_1.png' alt='prototype' width='400px' style='padding-left: 130px;'/>\n\t\t\t\n\t\t\t<p>Spinning from the center, the Smart Travel project was created. The goal of the smart travel project is to make cars talk! A growing number of auto manufacturers seek to embed location awareness systems (lane detection, blind spot detection, etc) into their vehicles. While this provides a means of observation, there is not yet a platform for collaboration or communication between vehicles; the smart travel project seeks to provide this foundation.\t</p>\n\t\t\t<img src='/images/project_pics/smarttravel_2.png' alt='prototype' width='400px' style='padding-left: 130px;'/>", 
            "largePic": "nav/smartlighting_lrg.jpeg", 
            "order": 1, 
            "slug": "smartlighting", 
            "smallPic": "nav/smartlighting.jpeg", 
            "summary": "Wireless communications using LED lights", 
            "title": "smartLighting", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>The <a href='http://www.bu.edu/nano' target='blank'>Nanomaterials and Nanostructure Optics Group</a>, led by Luca Dal Negro focuses on exploring subdiffusive transport of photons in aperiodic nanostructure lattices.</p>\n\t\t\t\n\t\t\t<img src='/images/project_pics/RW2D_PDF.jpeg' style='padding-left: 10px'/>\t\n\t\t\t\n\t\t\t<p>My work was inspired by the recent excitement in the photonic community regarding anomalous transport of light through engineered materials. This enthusiasm has focused largely on superdiffusion while the realm of subdiffusion has been left largely unexplored. Recognizing this void in understanding, my thesis provides some of the first study into the domain of subdiffusive transport in deterministic photonic systems. Such a phenomenon is of great interest to the development of advanced thin-film solar cells and optical sensing technologies which depend heavily on multiple scattering phenomena and the engineering of slow light.</p>\n\t\t\t\n\t\t\t<p>My thesis can be <a href='/static/files/travisrich_thesis.pdf' target='blank'>downloaded here.</a></p>\n\t\t\t\n\t\t\t<img src='/images/project_pics/TM_waveguide.jpg' width='400px' style='padding-left: 100px'/>", 
            "largePic": "nav/nano_lrg.png", 
            "order": 2, 
            "slug": "nano", 
            "smallPic": "nav/nano.jpeg", 
            "summary": "Sub-diffusive photon transport in aperiodic nanostructures", 
            "title": "nano", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>Beginning in May 2013, I joined the Government of Andorra and became the Director of Technology for their Smart Country program. The program is seeking to revitalize the countries economy by introducing new technologies and infrastructure. </p>\n\n<p>From the Government, the Smart Country program is championed by the Secretary of State for Economic Diversification, Josep Maria Miss\u00e9 and Minister of Foreign Affairs, Gilbert Saboya Suny\u00e9. I have been brought in to lead the development of the 1, 2, and 5-year strategic and technical planning of the project, as well as to serve as the bridge between the telco, energy company, local startups, and international agencies in support of the program.</p>\n<img src='/images/project_pics/andorra_1.jpg' width='600px'/>\n<p>If Andorra is anything, it\u2019s small. They have a single government-owned telecom company, a single government-owned energy utility provider, and a 1km strip of road through which 90% of the 8,000,000 annual tourists pass. With this small size, concentrated technology sector, and agile government, Andorra is seeking to launch the world\u2019s first fully-connected, fully-integrated Smart Country program.</p>\n<img src='/images/project_pics/andorra_2.jpg' width='600px'/>\n\n<p>A beautiful writeup, done by Gabe Stein can be found on <a href='fastcolabs.com/3013432/state-owned-gold-mine-what-happens-when-governments-sell-data' target='blank'>FastCompany</a>. More coming soon...</p>", 
            "largePic": "nav/andorra_lrg.jpg", 
            "order": 16, 
            "slug": "openAndorra", 
            "smallPic": "nav/andorra.jpg", 
            "summary": "openAndorra", 
            "title": "openAndorra", 
            "z_subProjects": []
        }, 
        {
            "html": "<p>An animated gif is a magical thing. It contains the power to convey emotion, empathy, and context in a subtle way that text or emoticons simply can't.</p>\n\n<p>GIFGIF is a project to capture that magic with quantitative methods. Our goal is to create a tool that lets people explore the world of gifs by the emotions they evoke, rather than by manually entered tags.</p>\n\n<a href='http://gifgif.media.mit.edu' target='_blank'><p>Play with the project at gifgif.media.mit.edu</p></a>\n\t\t\t\n<p>The project owes much inspiration to the <a target='blank' href='http://pulse.media.mit.edu'>Place Pulse</a> project and its team.</p>\n\n<img src='/images/project_pics/gifgif_00.png' width='600px'>\n<br/>\n\n<p>GIFGIF has been lovingly written by Travis Rich and Kevin Hu with much support from the grad students and faculty in the <a target='_blank' href='http://viral.media.mit.edu'>Viral Spaces</a> and <a target='_blank' href='http://macro.media.mit.edu'>Macro Connections</a> research groups. </p>\n\n<p>GIFGIF is built in hopes of answering some really interesting questions. Does a gif's emotional variance impact how it's received? (We have a hunch that emotional variance is why :) is pretty acceptable but ;) is typically an awkward mix of creepy/sexy/playful/pirate-y). Does a gif's emotional content vary between cultures? For example, what is the best representation of happiness for Germans, compared to a Canadian's impression?</p>\n\n<img src='/images/project_pics/gifgif_01.png' width='600px'>\n<br/>\n\n<p>We build this project with the caveat that emotions are tricky\u2013and by no means a trivial (or even possible) thing to quantify. We seek to use emoticons that represent a 'core' or 'basic' emotion so we can understand the atomistic elements of what a gif is conveying. For such a complex goal, we defer to the breadth of research and career-long effort of <a href='http://en.wikipedia.org/wiki/Paul_Ekman' target='_blank'>Paul Ekman</a>. In some of his earlier work he found six emotions to be universal and basic: anger, disgust, fear, happiness, sadness, and surprise. In the 90s, he expanded this selection of universal emotions to include the 17 you find right here on GIFGIF.</p>\n<p>These are by no means definitive, correct, or absolute, but our goal is to make a gif website\u2013not solve the open questions of universal human emotions. Some reading for those interested: <a target='_blank' href='http://www.paulekman.com/wp-content/uploads/2013/07/Basic-Emotions.pdf'>[1]</a> <a target='_blank' href='http://psycnet.apa.org/journals/psp/17/2/124/'>[2]</a></p> \n\n\n<h4>Press</h4>\n<ul>\n\t<a href='http://www.wired.co.uk/news/archive/2014-03/06/mit-map-language-of-gifs' target='_blank'><li>Wired.co.uk</li></a>\n        <a href='http://www.buzzfeed.com/ariannarebolini/map-the-emotional-language-of-gifs-with-mits-gifgif-project' target='_blank'><li>Buzzfeed</li></a>\n\t<a href='http://qz.com/185573/these-mit-researchers-want-to-translate-shakespeare-into-gifs/' target='_blank'><li>Quartz</li></a>\n\t<a href='http://www.fastcodesign.com/3027467/mit-students-invent-a-universal-language-made-of-gifs' target='_blank'><li>FastCo Design</li></a>\n\t<a href='http://www.theverge.com/2014/3/11/5496890/are-animated-gifs-a-language-mit-student-project-aims-to-find-out' target='_blank'><li>The Verge</li></a>\n\t<a href='http://io9.com/this-gif-translation-game-will-take-over-your-day-1542132553' target='_blank'><li>io9</li></a>\n\t<a href='http://www.theatlantic.com/education/archive/2014/03/these-mit-researchers-want-to-turn-gifs-into-a-language/284322/' target='_blank'><li>Atlantic</li></a>\n\t<a href='http://time.com/18172/these-students-want-to-turn-gifs-into-their-own-language/' target='_blank'><li>Time</li></a>\n\t<a href='http://www.geekosystem.com/gifgifgame/' target='_blank'><li>GeekoSystem</li></a>\n\t<a href='http://www.boston.com/news/source/2014/03/get_excited_mit_students_are_conducting_the_gif-iest_study_a.html' target='_blank'><li>Boston.com</li></a>\n\n\n</ul>", 
            "largePic": "nav/gifgif_lrg.jpg", 
            "order": 23, 
            "slug": "gifgif", 
            "smallPic": "nav/gifgif.jpg", 
            "summary": "An animated gif is a magical thing. It contains the power to convey emotion, empathy, and context in a subtle way that text or emoticons simply can't. GIFGIF is a project to capture that magic with quantitative methods. Our goal is to create a tool that lets people explore the world of gifs by the emotions they evoke, rather than by manually entered tags.", 
            "title": "GIFGIF", 
            "z_subProjects": []
        }
    ]
};

export default projects;